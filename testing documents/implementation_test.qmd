```{r}
#necessary libraries
library(tidyverse)
library(quantreg)
library(MASS)
```

For simplicity, we are in the case where we don't need multiple datasets, just two
*these functions are from data_generating_mechanism.qmd*

```{r}
#logit and expit functions for myself
logit <- function(prob) {
  value <- log(prob / (1 - prob))
  return(value)
}

expit <- function(prob) {
  value <- 1 / (1 + exp(-(prob)))
  return(value)
}

#now some other transformation functions
log_quant_transform <- function(value, min, max) {
  new_value <- log((value - min) / (max - value))
  
  if (is.nan(new_value)) {return(NA)} 
  else {return(new_value)}
}

inv_log_quant_transform <- function(value, min, max) {
  new_value <- (exp(value)*max + min) / (1+exp(value))
  return(new_value)
}
```

```{r}
coefficient_generator <- function(tau, num_studies, 
                                  a_0 = logit(0.3), a_1 = log(1.1), 
                                  b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2)) {
    
  #create which number study we have
  study_counts <- data.frame(id = 1:num_studies)
  
  study_counts <- study_counts |>
    mutate(study_num = paste("Study", id)) |>
    dplyr::select(study_num)
  
  #allow a_1 to vary
  alpha_coefficients <- rnorm(num_studies, a_1, tau)
  alpha_coefficients <- as.data.frame(alpha_coefficients) #make into data frame
  
      #renaming for better binding experience + adding a_0... not important here
      alpha_coefficients <- alpha_coefficients |> 
        mutate(a_0 = a_0) |>
        rename(a_1 = "alpha_coefficients") |>
        dplyr::select(a_0, a_1) #reordering
  
      
  #now doing the beta coefficients
  beta_vector <- c(b_1, b_2, b_3)
  
    #variance-covariance matrix
    matrix_size <- length(beta_vector)
    diag_mat <- matrix(0, matrix_size, matrix_size) 
    diag(diag_mat) <- 1 #make all the diagonals 1 for the identity matrix
    
    #lastly, we need to perform the calculation specified in section 3.1.4
    beta_matrix <- tau * diag_mat
    beta_coefficients <- mvrnorm(num_studies, beta_vector, beta_matrix)
    beta_coefficients <- as.data.frame(beta_coefficients)
    
      #renaming for better binding experience
      beta_coefficients <- beta_coefficients |> 
        mutate(b_0 = b_0) |>
        rename(b_1 = "V1", b_2 = "V2", b_3 = "V3") |>
        dplyr::select(b_0, b_1, b_2, b_3)
      
  #we need to set a LoD value for these studies that has slight variation
  LoD <- data.frame(LoD = round(runif(num_studies, min = 0.8, max = 1.2), 3))
  
  #lastly, we need a range of degrees of freedom for the confounder generation
  #there will be two just in case I want to use an F distribution
  deg_freedom <- data.frame(df_1 = round(runif(num_studies, min = 3, max = 7)),
                            df_2 = round(runif(num_studies, min = 3, max = 7)))
  
  #now combine these results
  coefficients <- cbind(study_counts, alpha_coefficients, beta_coefficients, LoD, deg_freedom)
  
  return(coefficients)
}

#coefficient_generator(tau = 0.01, num_studies = 30)
```

```{r}
#this will be nested to make the list function cleaner to read
create_data <- function(sample_size, a_0, a_1, b_0, b_1, b_2, b_3, df_1, df_2) {
  
  #initialize everything
  n <- sample_size
  
  #step 1: Base Binary Predictor
  V <- rbinom(n, size = 1, prob = 0.4)
  
  #step 2: cnfounder with a skewed distribution... Biomarker missing
  C1 <- rchisq(n, df = df_1) 
  C2 <- rnorm(n, mean = 75, sd = 7) #another confounder with a normal distribution. not used right now
  
  #step 3: generating exposure variable based on confounders (probability)
  E <- expit(a_0 + a_1*C1)
  
  #step 4: generating outcome based on confounders, exposure, and base binary predictor
  O <- expit(b_0 + b_1*E + b_2*V + b_3*C1)
  
  #step 5: create dataset
  
  my_data <- data.frame(
    predictor = V,
    confounder_1 = C1,
    confounder_2 = C2,
    exposure = E,
    outcome = O
  )
  
  return(my_data)
} 
```

```{r}
#recall all coefficients are stored in coefficient_generator funct
create_multiple_datasets <- function(study_coefficient_dataset, sample_size) {
  my_list <- list() #initialize empty list
  
  #pulling all values we need
  a_0_values <- study_coefficient_dataset |> dplyr::pull(a_0)
  a_1_values <- study_coefficient_dataset |> dplyr::pull(a_1)
  b_0_values <- study_coefficient_dataset |> dplyr::pull(b_0)
  b_1_values <- study_coefficient_dataset |> dplyr::pull(b_1)
  b_2_values <- study_coefficient_dataset |> dplyr::pull(b_2)
  b_3_values <- study_coefficient_dataset |> dplyr::pull(b_3)
  
  #also degrees of freedom
  df_1_values <- study_coefficient_dataset |> dplyr::pull(df_1)
  df_2_values <- study_coefficient_dataset |> dplyr::pull(df_2) #only if necessary
  
  for(i in 1:nrow(study_coefficient_dataset)) {
    #do the data generating mechanism
    a_0 <- a_0_values[i]
    a_1 <- a_1_values[i]
    b_0 <- b_0_values[i]
    b_1 <- b_1_values[i]
    b_2 <- b_2_values[i]
    b_3 <- b_3_values[i]
    
    #also the degrees of freedom
    df_1 <- df_1_values[i]
    df_2 <- df_2_values[i]
    
    #apply the data generating mechanism function from the values above
    my_data <- create_data(sample_size = sample_size, 
                           a_0 = a_0, a_1 = a_1, 
                           b_0 = b_0, b_1 = b_1, b_2 = b_2, b_3 = b_3,
                           df_1 = df_1, df_2 = df_2)
    
    
    #finally, add this dataset to our list
    my_list[[i]] <- my_data
  
  }
  
  #return the list
  return(my_list)
}
```



Generate my data now

```{r}
set.seed(605)

#don't forget to load the three functions necessary to make this work
my_coefficients_basic <- coefficient_generator(tau = 0.01, num_studies = 2)
my_studies_basic <- create_multiple_datasets(my_coefficients_basic, sample_size = 500)
```

```{r}
#doesn't matter which study is the basis study
basis_study <- as.data.frame(my_studies_basic[1])
missing_study <- as.data.frame(my_studies_basic[2])

#get rid of the mess
rm(my_studies_basic)
```

Now simple mechanism to make LoD data for the missing study. Also count the prop of missing data

```{r}
#look at my_coefficients and see that randomly selected LoD is 0.912
missing_study <- missing_study |>
  mutate(confounder_1_missing = ifelse(confounder_1 >= 0.912, confounder_1, NA))

#count proportion of missingness here: 17.6%
prop_missing = sum(is.na(missing_study$confounder_1_missing)) / nrow(missing_study)
prop_missing
```

View what the confounder 1 looks like

```{r}
ggplot(missing_study) +
  geom_histogram(aes(x = confounder_1, fill = "original"), alpha = 0.5) +
  geom_histogram(aes(x = confounder_1_missing, fill = "imputed"), alpha = 0.5) + 
  xlim(0,10)
```


`STEP 1`: Perform a logistic transform on the Biomarker to impute.

```{r}
min = 0
max = ceiling(quantile(basis_study$confounder_1, prob = 0.99))

#the transformation of confounder 1
basis_study <- basis_study |>
  mutate(confounder_1_transformed = sapply(confounder_1, log_quant_transform, min, max))

#some NaN's are produced, but that is to be expected
```

`STEP 2`: Perform Conditional Logistic Quantile Regression

```{r}
#test with 1st quartile, median, and 3rd quartile: need to transform them back

#initiliaze empty data frame
coefficient_data <- data.frame()

for(i in seq(0.01:0.99, by=0.01)) {
  reg_coeff <- rq(confounder_1_transformed ~ exposure + outcome, data = basis_study, tau=i)
  
  new_data <- data.frame(
    b0 = reg_coeff$coefficients[1],
    b1 = reg_coeff$coefficients[2],
    b2 = reg_coeff$coefficients[3],
    quant = i
  )
  
  coefficient_data <- rbind(coefficient_data, new_data) #add to new iterations
}
```

```{r, eval=FALSE}
reg_test <- rq(confounder_1_transformed ~ exposure + outcome, data = basis_study, tau=0.44)
summary(reg_test) #is there a way to extract variance here?
```

`STEP 2.5`: Store these results in a data frame

```{r}
#we already did this, so we're just going to clean up
rm(new_data)
rm(reg_coeff)

coefficient_data
```


`STEP 3`: Send these coefficients to an agreed-upon central study site.
- Some extra steps here, but we are keeping it at |A| = 1 for now
`STEP 4`: Send the list of regression coefficients to Missing-Data Datasets. 
`STEP 4.5`: Each individual Missing-Data Study calculates FMj, and round this value to the nearest thousandths place. 

```{r}
#we had this code previously, but it's repeated here for clarification
prop_missing = sum(is.na(missing_study$confounder_1_missing)) / nrow(missing_study)
prop_missing
```

`STEP 5`: Perform the Imputation Algorithm






Random pick from u ~ U(0,1)
Get the Floor and Modulus of u
Some weighting thing with the quantile regression coefficients
Step 6: Transform these LD variables back to regular variables 
Min = 0 
Max = ⌈QFMj(CB)⌉ of the missing-data study



`STEP 7`: Perform logistic regression (or anything else you’re looking at) using this newly imputed variable and obtain the parameter of interest

`STEP 8`: Repeat Steps 5-7 a total of 10 times (as per Multiple Imputation standards). Store the desired parameter and its variance-covariance matrix


`STEP 9`: Aggregate with Rubin’s Rules


***************************************************************************************************************

Now we need to analyze the results, which will be another subsantive section in this document
