```{r}
library(tidyverse)
library(missMethods)
library(quantreg)
```

Trying Logistic Quantile Regression: clear R history for this to work properly

```{r}
#initial data generation
set.seed(2025)

#skewed distribution for imputation testing
my_data <- data.frame(
  original_data = rchisq(1000, df = 5)) #can change to any other distribution: try F and gamma?

#rchisq(1000, df = 5)
#rgamma(1000, 4, rate = 3)
#rf(1000, 6, 4)

#rnorm(1000, mean = 0, sd = 4)

ggplot(data = my_data, aes(x=original_data)) +
  geom_density()
```

```{r}
#the bounds for our variable: this is just a toy example
min <- 0
max <- 14 #for the sake of demonstration, let's create this cutoff
count(my_data |> filter(original_data > 14)) #as we can see, 10 of the 1000 obs are out of the range

my_data <- my_data |>
  mutate(transformed_data = log((original_data - min) / (max - original_data)))

ggplot(data = my_data, aes(x=transformed_data)) +
  geom_density()
```

```{r}
#mechanism to make missing data (from https://cran.r-project.org/web/packages/missMethods/vignettes/Generating-missing-values.html)
make_missing <- delete_MCAR(my_data, 0.5, "original_data") #half the data is gone now

#likely a better way to approach this, but I'm lazy. R has gotten better with for loops
for(i in 1:nrow(make_missing)) {
  if(is.na(make_missing$original_data[i])) {
    make_missing$transformed_data[i] <- NA
  }
}
```

```{r}
#conditional quantile function with my_data and transformed_data!
imputation_process_logistic <- function(full_data) {
  suppressWarnings({ #thanks stackexchange
    unif_value <- round(runif(1, min = 0, max = 1), 2) #sample from Unif(0,1) and round to hundredths
    quant_reg_coeff <- rq(transformed_data ~ 1, data = full_data, tau=unif_value) #why is this 1?
    value <- coef(quant_reg_coeff)[1]
  })
  return(value)
}
```

```{r}
#this time, the fitted values can only be within a certain range, so when un-transforming it, we should never observe a value outside of (min, max)

for (i in 1:nrow(make_missing)) {
  if (is.na(make_missing$transformed_data[i])) {
    quantile_value <- imputation_process_logistic(my_data) #from basis dataset
    make_missing$imputed_data[i] <- quantile_value
  }
  else {
    make_missing$imputed_data[i] <- make_missing$transformed_data[i]
  }
}
```

```{r}
#untransform the data
make_missing <- make_missing |>
  mutate(untransformed_imputed_data = (exp(imputed_data)*max + min) / (1+exp(imputed_data)))
```

```{r}
#consistency
my_data <- my_data |>
  dplyr::select(original_data) |>
  mutate(type = "original") |>
  rename(data = "original_data")

make_missing <- make_missing |>
  dplyr::select(untransformed_imputed_data) |>
  mutate(type = "imputed") |>
  rename(data = "untransformed_imputed_data")

#concatenating
comparison <- rbind(my_data, make_missing)
rm(my_data)
rm(make_missing)

#the plot: the imputation technique works!
ggplot(comparison, aes(data, fill = type)) + 
  geom_density(alpha = 0.2)

#quick note: changing max to a value like 6 shows that the limited range works!
#it does show that we need to be very careful in choosing our min and max values
```

## Systematically Missing Imputation: 
Even with 100% missing data from a limited range variable, this method still works decently

```{r}
#playing around with some ideas:
min <- 0
max <- 14

#limit of detection problems
left_limit <- 1
right_limit <- 14

#some generalized calculations
left_lim_dist <- left_limit - min #the distance of the limit of detection
right_lim_dist <- max - right_limit
```

```{r}
#say we had code to show that from 0 to 1 we cannot detect
test_data <- data.frame(
  original_data = rf(1000, 6, 4)) #can change to any other distribution: try F and gamma?

test_data <- test_data |>
  mutate(limited_data = ifelse(original_data > left_limit, original_data, NA)) #systematically missing

#from here, you create a subset of the dataset in this manner
test_data <- test_data |>
  mutate(left_basis = ifelse(original_data <= left_limit, original_data, NA)) #the basis for the limited range

#we probably need over 100 observations to create an accurate enough basis, but we can check this further
sum(!is.na(test_data$left_basis)) 
```

```{r}
#similar to previous function, but using left_basis
imputation_process_limited <- function(limited_data) {
  suppressWarnings({ #thanks stackexchange
    unif_value <- round(runif(1, min = 0, max = 1), 2) #sample from Unif(0,1) and round to hundredths
    quant_reg_coeff <- rq(left_basis ~ 1, data = limited_data, tau=unif_value)
    value <- coef(quant_reg_coeff)[1]
  })
}

#similar for loop we did before
for (i in 1:nrow(test_data)) {
  if (is.na(test_data$limited_data[i])) {
    quantile_value <- imputation_process_limited(test_data) #from basis dataset
    test_data$limited_data[i] <- quantile_value
  }
  else {
    test_data$limited_data[i] <- test_data$limited_data[i]
  }
}

#wow it works decently well... should do this 1000 times to confirm
ggplot(test_data) +
  geom_density(aes(x = original_data, fill = "original"), alpha = 0.5) +
  geom_density(aes(x = limited_data, fill = "imputed"), alpha = 0.5) + 
  xlim(0,10)

ggplot(test_data) +
  geom_density(aes(x = original_data, fill = "original"), alpha = 0.5) +
  geom_density(aes(x = limited_data, fill = "imputed"), alpha = 0.5) + 
  xlim(0,1)
```








## Imputing with a Basis Study instead of from the same study... allow for some variation in the generation

```{r}
#an idea that I just had
#say that we have a case where 0 to 1 is systematically missing and 1 to 14 is detected
#look at the range of a variable from the basis dataset
#calculate the quantile proportions that we need to impute and divide it by 100
#in this case, the quantiles from 0 to 1 is 0.01 to 0.06. generate 99 equidistant quantiles here

#then using the basis dataset, impute all missing values from 0 to 1 using these quantiles
#profit?

#did I miss something?

```

```{r}
#logit transformation and its inverse as a function

log_quant_transform <- function(value, min, max) {
  new_value <- log((value - min) / (max - value))
  
  if (is.nan(new_value)) {return(NA)} 
  else {return(new_value)}
}

inv_log_quant_transform <- function(value, min, max) {
  new_value <- (exp(value)*max + min) / (1+exp(value))
  return(new_value)
}
```

```{r}
#imputation function
imputation_process <- function(limited_data) {
  suppressWarnings({ #thanks stackexchange
    unif_value <- round(runif(1, min = 0, max = 1), 2) #sample from Unif(0,1) and round to hundredths
    quant_reg_coeff <- rq(left_basis ~ 1, data = limited_data, tau=unif_value)
    value <- coef(quant_reg_coeff)[1]
  })
}
```

```{r}
#create our data
set.seed(495)
sample_size <- 1000
study_A  <- data.frame(A_data = rchisq(sample_size, df = 3))
study_B <- data.frame(B_data = rchisq(sample_size, df = 3)) #the basis study

#for study B, take out all values below the 25th quantile
quarter_quantile_B <- as.double(quantile(study_B$B_data, 0.25))

study_B <- study_B |>
  mutate(B_data_missing = ifelse(B_data > quarter_quantile_B, B_data, NA))
  
#seeing the missing data through histogram
ggplot(study_B) +
  geom_histogram(aes(x = B_data, fill = "original"), alpha = 0.5) +
  geom_histogram(aes(x = B_data_missing, fill = "missing"), alpha = 0.5)
```

```{r}
#another exploratory plot... looking at study A and B's histograms (thanks ChatGPT for making this less painful)
combined_data <- data.frame(
  value = c(study_A$A_data, study_B$B_data),
  study = rep(c("Study A", "Study B"), times = c(length(study_A$A_data), length(study_B$B_data)))
)

# Create the plot
ggplot(combined_data, aes(x = value, fill = study)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 30, color = "black") +
  scale_fill_manual(values = c("skyblue", scales::alpha("red", 0.5))) +
  geom_vline(xintercept  = quarter_quantile_B) + #this is where the data will go missing... 
  theme_minimal() +
  theme(legend.title = element_blank()) 
```

```{r}
#from study B, get the min and max for missing values... send that information to study A
min <- 0
max <- quarter_quantile_B
quantile_from_B <- 0.25 #calculate prop of missing values and that's the quantile value to give us

#study A does some transformation nonsense to send a lot of detailed information for within this range
study_A <- study_A |>
  mutate(A_data_transformed = sapply(A_data, log_quant_transform, min = min, max = max))

#now storing the coefficients we need 
A_coeff_transformed <- c()

for(i in seq(0, quantile_from_B, by = 0.01)) {
  quant_reg_coeff <- rq(A_data_transformed ~ 1, data = study_A, tau=i)
  iteration <- i*100 + 1
  A_coeff_transformed[iteration] <- coef(quant_reg_coeff)[1]
}
```

```{r}
#send the information over
imputation_process <- function(B_dataset, basis_coefficients, min, max) {
  suppressWarnings({ #thanks stackexchange
    unif_value_adj <- rbeta(1, shape1 = 12, shape2 = 9) / (1/quantile_from_B)*100 #from 1 to 26 in our case
    unif_value_adj <- round(unif_value_adj) #just round
    
    #getting value
    value <- basis_coefficients[unif_value_adj]
    value <- inv_log_quant_transform(value, min, max)
    
    return(value)
    
  })
}

#similar for loop we did before
for (i in 1:nrow(study_B)) {
  if (is.na(study_B$B_data_missing[i])) {
    quantile_value <- imputation_process(study_B, A_coeff_transformed, min, max) #from basis dataset
    study_B$B_data_missing[i] <- quantile_value
  }
  else {
    study_B$B_data_missing[i] <- study_B$B_data_missing[i]
  }
}

ggplot(study_B) +
  geom_histogram(aes(x = B_data, fill = "original"), alpha = 0.5) +
  geom_histogram(aes(x = B_data_missing, fill = "missing"), alpha = 0.5)
```

```{r}
#looks terrible, maybe we can use a beta distribution, with beta(1,1) as a starting point since that's a uniform

rbeta(1, shape1 = 2, shape2 = 3) #a bit more skewed?
```

