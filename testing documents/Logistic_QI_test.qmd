```{r}
library(tidyverse)
library(missMethods)
library(quantreg)
```

Important functions I will be using

```{r}
set.seed(495)

log_quant_transform <- function(value, min, max) {
  new_value <- log((value - min) / (max - value))
  
  if (is.nan(new_value)) {return(NA)} 
  else {return(new_value)}
}

inv_log_quant_transform <- function(value, min, max) {
  new_value <- (exp(value)*max + min) / (1+exp(value))
  return(new_value)
}
```


## Systematically Missing Imputation: Delete all available data from a certain range. In this case, (0, 1)

Initializing important information

```{r}
#playing around with some ideas:
min <- 0
max <- 14

#limit of detection values
left_limit <- 1
right_limit <- 14
```

Generating our data

```{r}
#say we had code to show that from 0 to 1 we cannot detect
test_data <- data.frame(
  original_data = rf(1000, 6, 4)) #can change to any other distribution: try F and gamma?

ggplot(data=test_data, aes(x=original_data)) + geom_density() + geom_vline(xintercept = left_limit)

test_data <- test_data |>
  mutate(limited_data = ifelse(original_data > left_limit, original_data, NA)) #systematically missing

#from here, you create a subset of the dataset in this manner
test_data <- test_data |>
  mutate(left_basis = ifelse(original_data <= left_limit, original_data, NA)) #the basis for the limited range

#we probably need over 100 observations to create an accurate enough basis, but we can check this further
sum(!is.na(test_data$left_basis)) 
```

This is where the logistic part comes in...

```{r}
test_data <- test_data |>
  mutate(left_basis_transformed = sapply(left_basis, log_quant_transform, min = min, max = max))
```

The algorithm

```{r}
#similar to previous function, but using left_basis
imputation_process_limited <- function(limited_data) {
  suppressWarnings({ #thanks stackexchange
    unif_value <- round(runif(1, min = 0, max = 1), 2) #sample from Unif(0,1) and round to hundredths
    quant_reg_coeff <- rq(left_basis_transformed ~ 1, data = limited_data, tau=unif_value)
    value <- coef(quant_reg_coeff)[1]
  })
}

#similar for loop we did before
for (i in 1:nrow(test_data)) {
  if (is.na(test_data$limited_data[i])) {
    quantile_value <- imputation_process_limited(test_data) #from basis dataset
      quantile_value <- inv_log_quant_transform(quantile_value, min, max) #extra bit to untransform the value
    test_data$limited_data[i] <- quantile_value
  }
  else {
    test_data$limited_data[i] <- test_data$limited_data[i]
  }
}
```

Checking to see imputted values are reasonable... they are!

```{r}
#wow it works decently well... should do this 1000 times to confirm
ggplot(test_data) +
  geom_density(aes(x = original_data, fill = "original"), alpha = 0.5) +
  geom_density(aes(x = limited_data, fill = "imputed"), alpha = 0.5) + 
  xlim(0,10)

ggplot(test_data) +
  geom_histogram(aes(x = original_data, fill = "original"), alpha = 0.5) +
  geom_histogram(aes(x = limited_data, fill = "imputed"), alpha = 0.5) + 
  xlim(0,1)
```







## Imputing with a Basis Study instead of from the same study... allow for some variation in the generation


```{r}
#imputation function
imputation_process_limited <- function(limited_data) {
  suppressWarnings({ #thanks stackexchange
    unif_value <- round(runif(1, min = 0, max = 1), 2) #sample from Unif(0,1) and round to hundredths
    quant_reg_coeff <- rq(left_basis_transformed ~ 1, data = limited_data, tau=unif_value)
    value <- coef(quant_reg_coeff)[1]
  })
}
```

"Data generating mechanism" made simple. In the future, Study A and Study B can have degrees of freedom that vary a lot
Sensitivity Analysis to see how different our population distributions can be before this method breaks down

```{r}
#create our data
sample_size <- 1000
study_A  <- data.frame(A_data = rchisq(sample_size, df = 5)) #the basis study
study_B <- data.frame(B_data = rchisq(sample_size, df = 3)) #rf(sample_size, 6, 8)

#simulation scenario: data from 0 to 1 in study B is missing, but A is all observed
min <- 0
max <- 14

left_limit <- 1
right_limit <- 14

study_B <- study_B |>
  mutate(B_data_missing = ifelse(B_data >= 1, B_data, NA))
  
#seeing the missing data through histogram
ggplot(study_B) +
  geom_histogram(aes(x = B_data_missing, fill = "missing"), alpha = 0.5) +
  geom_histogram(aes(x = B_data, fill = "original"), alpha = 0.2) +
  xlim(0,20)
```

What we need to do is create a basis of imputation with Study A. 
This means Study B needs to send their left and right limits to Study A
Make sure that this value is also transformed with the log scale based on the min and max given
*Study B should also send proportion of data missing to get the quantiles necessary*

```{r}
study_A <- study_A |>
  mutate(left_basis = ifelse(A_data < 1, A_data, NA)) |>
  mutate(left_basis_transformed = sapply(left_basis, log_quant_transform, min, max))
```

Then, Study A needs to do the quantile regression algorithm and store the quantiles in a vector to send to study B
*an idea here: we should calculate up to what quantile is Study B missing data in*
*then use that for the number of iterations we do right now*
*this is what Nicola did, but for now, since it works, keep this*

```{r}
study_A_coefficients <- c() #initialize vector

for(i in seq(0, 0.99, by = 0.01)) {
  quant_reg_coeff <- rq(left_basis_transformed ~ 1, data = study_A, tau=i)
  iteration <- (i*100) + 1
  study_A_coefficients[iteration] <- coef(quant_reg_coeff)[1]
}

#TEMPORARY FIX
study_A_coefficients #not entirely sure why we have NA values... it's 30 and 59... nonunique solutions...
quant_30 <- rq(left_basis_transformed ~ 1, data = study_A, tau=0.29)
study_A_coefficients[30] <- coef(quant_30)[1]

quant_59 <- rq(left_basis_transformed ~ 1, data = study_A, tau=0.59)
study_A_coefficients[59] <- coef(quant_59)[1]

study_A_coefficients
```

Now that we have a basis of imputation, we send this to study B's site for them to apply the imputation algorithm

```{r}
study_B <- study_B |>
  mutate(imputation_indicator = NA)

for (i in 1:nrow(study_B)) {
  if (is.na(study_B$B_data_missing[i])) { #if we have a missing value, apply this algorithm
    unif_value <- runif(1, min = 0, max = 0.99) * 100
    unif_value <- round(ceiling(unif_value)) #if you don't do ceiling you might get 0, and that's no good
    
    #we can now obtain the value, and also untransform it
    value <- study_A_coefficients[unif_value]
    value <- inv_log_quant_transform(value, min, max)
    
    #then we impute
    study_B$B_data_missing[i] <- value
    study_B$imputation_indicator[i] <- 1 #indicates imputed
  }
  else {
    study_B$B_data_missing[i] <- study_B$B_data_missing[i] #we use original value
    study_B$imputation_indicator[i] <- 0 #indicates not imputed
  }
}
```

```{r}
ggplot(study_B) +
  geom_density(aes(x = B_data, fill = "original"), alpha = 0.5) +
  geom_density(aes(x = B_data_missing, fill = "imputed"), alpha = 0.5) + 
  xlim(0, 3)
```


We can now try and compare this imputation from a different dataset with a naive estimate
*note that these datasets must contain an imputation_indicator variable*

```{r}
#this function adds constant imputations

naive_constant_imputation <- function(my_data, full_variable, min, max) {
  
  LoD <- max - min #this is the Limit of Detection range
  
  my_data <- my_data |>
    mutate(constant_sqrt2 = ifelse(imputation_indicator == 1, LoD / sqrt(2), !!sym(full_variable)),
           constant_2 = ifelse(imputation_indicator == 1, LoD / 2, !!sym(full_variable))) #LoD / sqrt(2)
  
  return(my_data)
  
}

naive_constant_imputation(study_B, "B_data", min = 0, max = 1)
```


```{r}
complete_case_analysis_estimate <- function(my_data, full_variable) {
  
  my_data <- my_data |>
    mutate(complete_case = ifelse(imputation_indicator == 1, NA, !!sym(full_variable)))
  
  return(my_data)
}

complete_case_analysis_estimate(study_B, "B_data")
```
