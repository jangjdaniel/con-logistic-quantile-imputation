b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2),
df_1, df_2) {
#initialize everything
n <- sample_size
#now an E
E_bin <- rbinom(n, size = 1, prob = 0.4)
#step 4: generating outcome based on confounders, exposure, and base binary predictor
my_O <- plogis(log(1.5) * E)
O_bin <- rbinom(n, size = 1, prob = my_O)
#step 5: create dataset
my_data <- data.frame(
exposure = E_bin,
outcome = O_bin
)
return(my_data)
}
create_data(sample_size = 1000)
#necessary libraries
library(tidyverse)
library(ggplot2)
library(quantreg)
library(MASS)
library(purrr)
library(tictoc)
#logit and expit functions for myself
logit <- function(prob) {
value <- log(prob / (1 - prob))
return(value)
}
expit <- function(x) {
value <- 1 / (1 + exp(-x))
return(value)
}
#now some other transformation functions
log_quant_transform <- function(value, min, max) {
new_value <- log((value - min) / (max - value))
if (is.nan(new_value)) {return(NA)} #negative log values return an NA. Will be important to address
else {return(new_value)}
}
inv_log_quant_transform <- function(value, min, max) {
new_value <- (exp(value)*max + min) / (1+exp(value))
return(new_value)
}
coefficient_generator <- function(tau, num_studies,
a_0 = logit(0.3), a_1 = log(1.1),
b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2)) {
#create which number study we have
study_counts <- data.frame(id = 1:num_studies)
study_counts <- study_counts |>
mutate(study_num = paste("Study", id)) |>
dplyr::select(study_num)
#allow a_1 to vary
alpha_coefficients <- rnorm(num_studies, a_1, tau)
alpha_coefficients <- as.data.frame(alpha_coefficients) #make into data frame
#renaming for better binding experience + adding a_0... not important here
alpha_coefficients <- alpha_coefficients |>
mutate(a_0 = a_0) |>
rename(a_1 = "alpha_coefficients") |>
dplyr::select(a_0, a_1) #reordering
#now doing the beta coefficients
beta_vector <- c(b_1, b_2, b_3)
#variance-covariance matrix
matrix_size <- length(beta_vector)
diag_mat <- matrix(0, matrix_size, matrix_size)
diag(diag_mat) <- 1 #make all the diagonals 1 for the identity matrix
#lastly, we need to perform the calculation specified in section 3.1.4
beta_matrix <- tau * diag_mat
beta_coefficients <- mvrnorm(num_studies, beta_vector, beta_matrix)
beta_coefficients <- as.data.frame(beta_coefficients)
#renaming for better binding experience
beta_coefficients <- beta_coefficients |>
mutate(b_0 = b_0) |>
rename(b_1 = "V1", b_2 = "V2", b_3 = "V3") |>
dplyr::select(b_0, b_1, b_2, b_3)
#lastly, we need a range of degrees of freedom for the confounder generation
#there will be two just in case I want to use an F distribution
if(tau == 0) { #common
deg_freedom <- data.frame(df_1 = round(runif(num_studies, min = 5, max = 5)), #don't touch it if it aint broke
df_2 = round(runif(num_studies, min = 5, max = 5)))
} else { #heterogenous
deg_freedom <- data.frame(df_1 = round(runif(num_studies, min = 3, max = 7)),
df_2 = round(runif(num_studies, min = 3, max = 7)))
}
#now combine these results
coefficients <- cbind(study_counts, alpha_coefficients, beta_coefficients, deg_freedom)
return(coefficients)
}
#this will be nested to make the list function cleaner to read
create_data <- function(sample_size,
a_0 = logit(0.3), a_1 = log(1.1),
b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2),
df_1, df_2) {
#initialize everything
n <- sample_size
#step 1: Base Binary Predictor
V <- rbinom(n, size = 1, prob = 0.4)
#step 2: confounder with a skewed distribution... Biomarker missing
C1 <- rchisq(n, df = df_1)
#C2 <- rnorm(n, mean = 75, sd = 7) #another confounder with a normal distribution. not used right now
#step 3: generating exposure variable based on confounders (probability)
E <- plogis(a_0 + a_1*C1)
E_bin <- rbinom(n, size = 1, prob = E)
#step 4: generating outcome based on confounders, exposure, and base binary predictor
O <- plogis(b_0 + b_1*E_bin + b_2*V + b_3*C1)
O_bin <- rbinom(n, size = 1, prob = O)
#step 5: create dataset
my_data <- data.frame(
predictor = V,
confounder_1 = C1,
exposure = E_bin,
outcome = O_bin
)
return(my_data)
}
#recall all coefficients are stored in coefficient_generator funct
create_multiple_datasets <- function(study_coefficient_dataset, sample_size) {
my_list <- list() #initialize empty list
#pulling all values we need
a_0_values <- study_coefficient_dataset |> dplyr::pull(a_0)
a_1_values <- study_coefficient_dataset |> dplyr::pull(a_1)
b_0_values <- study_coefficient_dataset |> dplyr::pull(b_0)
b_1_values <- study_coefficient_dataset |> dplyr::pull(b_1)
b_2_values <- study_coefficient_dataset |> dplyr::pull(b_2)
b_3_values <- study_coefficient_dataset |> dplyr::pull(b_3)
#also degrees of freedom
df_1_values <- study_coefficient_dataset |> dplyr::pull(df_1)
df_2_values <- study_coefficient_dataset |> dplyr::pull(df_2)
for(i in 1:nrow(study_coefficient_dataset)) {
#do the data generating mechanism
a_0 <- a_0_values[i]
a_1 <- a_1_values[i]
b_0 <- b_0_values[i]
b_1 <- b_1_values[i]
b_2 <- b_2_values[i]
b_3 <- b_3_values[i]
#also the degrees of freedom
df_1 <- df_1_values[i]
df_2 <- df_2_values[i]
#apply the data generating mechanism function from the values above
my_data <- create_data(sample_size = sample_size,
a_0 = a_0,
a_1 = a_1,
b_0 = b_0,
b_1 = b_1,
b_2 = b_2,
b_3 = b_3,
df_1 = df_1,
df_2 = df_2)
#finally, add this dataset to our list
my_list[[i]] <- my_data
}
#return the list
return(my_list)
}
data_generating_mechanism <- function(sample_size, LD, tau) {
#first, create my studies based on coefficients
my_coefficients <- coefficient_generator(tau = tau, num_studies = 2) #2 is consistent here
my_studies <- create_multiple_datasets(my_coefficients, sample_size = sample_size)
#log transform our data
min = 0
max = ceiling(quantile(my_studies[[1]]$confounder_1, prob = 0.99))
suppressWarnings({ #a lot of warnings... some will be NA i know.
my_studies[[1]] <- my_studies[[1]] |>
mutate(confounder_1_transformed = sapply(confounder_1, log_quant_transform, min, max))
})
#define the LD and make data missing based on that
biomarker_values <- my_studies[[2]]$confounder_1
LoD_value <- as.numeric(quantile(biomarker_values, LD)) #must be given as a decimal
#now we made the data missing based on our given LD, and also transform this value
my_studies[[2]] <- my_studies[[2]] |>
mutate(confounder_1_missing = ifelse(confounder_1 >= LoD_value, confounder_1, NA))
#just to be able to look into my_coefficients later
my_studies <- list(my_studies[[1]], my_studies[[2]], my_coefficients)
return(my_studies) #return list of studies with missingness added
}
#confirm that my coefficients are working right (correct)
test_data <- data_generating_mechanism(sample_size = 500,
LD = 0.60,
tau = 0)
test_data[[3]]
missing_log <- glm(outcome ~ exposure + predictor + confounder_1,
data = as.data.frame(test_data[[2]]),
family = "binomial")
summary(missing_log)
#initialize
one_vect <- c()
two_vect <- c()
CC_vect <- c()
for(j in 1:1000) {
test_data <- data_generating_mechanism(sample_size = 5000,
LD = 0.60,
tau = 0)
one <- test_data[[1]]
two <- test_data[[2]]
one_log <- glm(outcome ~ exposure + predictor + confounder_1,
data = one,
family = "binomial")
two_log <- glm(outcome ~ exposure + predictor + confounder_1,
data = two,
family = "binomial")
one_vect[j] <- summary(one_log)$coefficients[2]
two_vect[j] <- summary(two_log)$coefficients[2]
twomiss_log <- glm(outcome ~ exposure + predictor + confounder_1_missing,
data = two,
family = "binomial")
CC_vect[j] <- summary(twomiss_log)$coefficients[2]
}
#not good... this should be centered around 0.405 aka ln(1.5)
hist(one_vect)
hist(two_vect)
hist(CC_vect)
mean(one_vect)
mean(two_vect)
mean(CC_vect)
log(1.5)
hist(CC_vect)
hist(two_vect)
hist(CC_vect)
for(j in 1:1000) {
test_data <- data_generating_mechanism(sample_size = 500,
LD = 0.60,
tau = 0)
one <- test_data[[1]]
two <- test_data[[2]]
one_log <- glm(outcome ~ exposure + predictor + confounder_1,
data = one,
family = "binomial")
two_log <- glm(outcome ~ exposure + predictor + confounder_1,
data = two,
family = "binomial")
one_vect[j] <- summary(one_log)$coefficients[2]
two_vect[j] <- summary(two_log)$coefficients[2]
twomiss_log <- glm(outcome ~ exposure + predictor + confounder_1_missing,
data = two,
family = "binomial")
CC_vect[j] <- summary(twomiss_log)$coefficients[2]
}
hist(two_vect)
hist(CC_vect)
#not good... this should be centered around 0.405 aka ln(1.5)
hist(one_vect)
hist(two_vect)
mean(two_vect)
mean(CC_vect)
mean(two_vect) - log(1.5)
mean(CC_vect) - log(1.5)
var(two_vect)
var(CC_vect)
#necessary libraries
library(tidyverse)
library(ggplot2)
library(quantreg)
library(MASS)
library(purrr)
library(tictoc)
#logit and expit functions for myself
logit <- function(prob) {
value <- log(prob / (1 - prob))
return(value)
}
expit <- function(x) {
value <- 1 / (1 + exp(-x))
return(value)
}
#now some other transformation functions
log_quant_transform <- function(value, min, max) {
new_value <- log((value - min) / (max - value))
if (is.nan(new_value)) {return(NA)} #negative log values return an NA. Will be important to address
else {return(new_value)}
}
inv_log_quant_transform <- function(value, min, max) {
new_value <- (exp(value)*max + min) / (1+exp(value))
return(new_value)
}
coefficient_generator <- function(tau, num_studies,
a_0 = logit(0.3), a_1 = log(1.1),
b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2)) {
#create which number study we have
study_counts <- data.frame(id = 1:num_studies)
study_counts <- study_counts |>
mutate(study_num = paste("Study", id)) |>
dplyr::select(study_num)
#allow a_1 to vary
alpha_coefficients <- rnorm(num_studies, a_1, tau)
alpha_coefficients <- as.data.frame(alpha_coefficients) #make into data frame
#renaming for better binding experience + adding a_0... not important here
alpha_coefficients <- alpha_coefficients |>
mutate(a_0 = a_0) |>
rename(a_1 = "alpha_coefficients") |>
dplyr::select(a_0, a_1) #reordering
#now doing the beta coefficients
beta_vector <- c(b_1, b_2, b_3)
#variance-covariance matrix
matrix_size <- length(beta_vector)
diag_mat <- matrix(0, matrix_size, matrix_size)
diag(diag_mat) <- 1 #make all the diagonals 1 for the identity matrix
#lastly, we need to perform the calculation specified in section 3.1.4
beta_matrix <- tau * diag_mat
beta_coefficients <- mvrnorm(num_studies, beta_vector, beta_matrix)
beta_coefficients <- as.data.frame(beta_coefficients)
#renaming for better binding experience
beta_coefficients <- beta_coefficients |>
mutate(b_0 = b_0) |>
rename(b_1 = "V1", b_2 = "V2", b_3 = "V3") |>
dplyr::select(b_0, b_1, b_2, b_3)
#lastly, we need a range of degrees of freedom for the confounder generation
#there will be two just in case I want to use an F distribution
if(tau == 0) { #common
deg_freedom <- data.frame(df_1 = round(runif(num_studies, min = 5, max = 5)), #don't touch it if it aint broke
df_2 = round(runif(num_studies, min = 5, max = 5)))
} else { #heterogenous
deg_freedom <- data.frame(df_1 = round(runif(num_studies, min = 3, max = 7)),
df_2 = round(runif(num_studies, min = 3, max = 7)))
}
#now combine these results
coefficients <- cbind(study_counts, alpha_coefficients, beta_coefficients, deg_freedom)
return(coefficients)
}
#this will be nested to make the list function cleaner to read
create_data <- function(sample_size,
a_0 = logit(0.3), a_1 = log(1.1),
b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2),
df_1, df_2) {
#initialize everything
n <- sample_size
#step 1: Base Binary Predictor
V <- rbinom(n, size = 1, prob = 0.4)
#step 2: confounder with a skewed distribution... Biomarker missing
C1 <- rchisq(n, df = df_1)
#C2 <- rnorm(n, mean = 75, sd = 7) #another confounder with a normal distribution. not used right now
#step 3: generating exposure variable based on confounders (probability)
E <- plogis(a_0 + a_1*C1)
E_bin <- rbinom(n, size = 1, prob = E) #USE THIS IN NEXT REGRESSION!!
#step 4: generating outcome based on confounders, exposure, and base binary predictor
O <- plogis(b_0 + b_1*E_bin + b_2*V + b_3*C1)
O_bin <- rbinom(n, size = 1, prob = O)
#step 5: create dataset
my_data <- data.frame(
predictor = V,
confounder_1 = C1,
exposure = E_bin,
outcome = O_bin
)
return(my_data)
}
#recall all coefficients are stored in coefficient_generator funct
create_multiple_datasets <- function(study_coefficient_dataset, sample_size) {
my_list <- list() #initialize empty list
#pulling all values we need
a_0_values <- study_coefficient_dataset |> dplyr::pull(a_0)
a_1_values <- study_coefficient_dataset |> dplyr::pull(a_1)
b_0_values <- study_coefficient_dataset |> dplyr::pull(b_0)
b_1_values <- study_coefficient_dataset |> dplyr::pull(b_1)
b_2_values <- study_coefficient_dataset |> dplyr::pull(b_2)
b_3_values <- study_coefficient_dataset |> dplyr::pull(b_3)
#also degrees of freedom
df_1_values <- study_coefficient_dataset |> dplyr::pull(df_1)
df_2_values <- study_coefficient_dataset |> dplyr::pull(df_2)
for(i in 1:nrow(study_coefficient_dataset)) {
#do the data generating mechanism
a_0 <- a_0_values[i]
a_1 <- a_1_values[i]
b_0 <- b_0_values[i]
b_1 <- b_1_values[i]
b_2 <- b_2_values[i]
b_3 <- b_3_values[i]
#also the degrees of freedom
df_1 <- df_1_values[i]
df_2 <- df_2_values[i]
#apply the data generating mechanism function from the values above
my_data <- create_data(sample_size = sample_size,
a_0 = a_0,
a_1 = a_1,
b_0 = b_0,
b_1 = b_1,
b_2 = b_2,
b_3 = b_3,
df_1 = df_1,
df_2 = df_2)
#finally, add this dataset to our list
my_list[[i]] <- my_data
}
#return the list
return(my_list)
}
data_generating_mechanism <- function(sample_size, LD, tau) {
#first, create my studies based on coefficients
my_coefficients <- coefficient_generator(tau = tau, num_studies = 2) #2 is consistent here
my_studies <- create_multiple_datasets(my_coefficients, sample_size = sample_size)
#log transform our data
min = 0
max = ceiling(quantile(my_studies[[1]]$confounder_1, prob = 0.99))
suppressWarnings({ #a lot of warnings... some will be NA i know.
my_studies[[1]] <- my_studies[[1]] |>
mutate(confounder_1_transformed = sapply(confounder_1, log_quant_transform, min, max))
})
#define the LD and make data missing based on that
biomarker_values <- my_studies[[2]]$confounder_1
LoD_value <- as.numeric(quantile(biomarker_values, LD)) #must be given as a decimal
#now we made the data missing based on our given LD, and also transform this value
my_studies[[2]] <- my_studies[[2]] |>
mutate(confounder_1_missing = ifelse(confounder_1 >= LoD_value, confounder_1, NA))
#just to be able to look into my_coefficients later
my_studies <- list(my_studies[[1]], my_studies[[2]], my_coefficients)
return(my_studies) #return list of studies with missingness added
}
logistic_quantile_regression_coefficients <- function(basis_study) {
coefficient_data <- data.frame() #initiliaze empty data frame
for(i in seq(from = 0.01, to = 0.99, by = 0.01)) {
reg_coeff <- rq(confounder_1_transformed ~ exposure + predictor + outcome, data = basis_study, tau=i)
new_data <- data.frame(
b0 = reg_coeff$coefficients[1],
b_exposure = reg_coeff$coefficients[2],
b_predictor = reg_coeff$coefficients[3],
b_outcome = reg_coeff$coefficients[4],
quant = i)
coefficient_data <- rbind(coefficient_data, new_data) #add to new iterations
}
return(coefficient_data)
}
#make this into a function for easy use?
find_prop_missing <- function(missing_data_study, LD) {
prop_missing <- sum(is.na(missing_data_study[[LD]])) / nrow(missing_data_study)
prop_missing <- ceiling(prop_missing * 100)
return(prop_missing)
}
uniform_values <- function(specified_max) {
u <- runif(1, min = 1, max = specified_max) #aka from 0.01 to 0.99
#all the values we need from the uniform
floor_u <- floor(u)
mod_u <- (u - floor(u))
next_u <- ceiling(u)
#now putting this into a vector
my_u <- c(u, floor_u, mod_u, next_u)
return(my_u)
}
imputation_algorithm <- function(basis_coefficients, study_for_imputation,
var_for_imputation, row_index, u_vector) {
u_vector <- u_vector
floor_quantile <- basis_coefficients[u_vector[2], ] #floor
ceiling_quantile <- basis_coefficients[u_vector[4], ] #ceiling
#need to calculate regression values... really messy don't look at this
lower_quantile_value <- floor_quantile$b0 +
(floor_quantile$b_exposure * study_for_imputation[row_index,]$exposure) + (floor_quantile$b_outcome * study_for_imputation[row_index,]$outcome)
upper_quantile_value <- ceiling_quantile$b0 +
(ceiling_quantile$b_exposure * study_for_imputation[row_index,]$exposure) + (ceiling_quantile$b_outcome * study_for_imputation[row_index,]$outcome)
modulus <- u_vector[3]
imputation_value_transformed <- ((1-modulus)*lower_quantile_value) + (modulus*upper_quantile_value)
#lastly, untransform this value using the right min and max... organization is a headache
min_imp <- 0
missing_data_proportion <- find_prop_missing(missing_data_study = study_for_imputation,
LD = as.character(var_for_imputation)) / 100
#THIS IS THE ISSUE...
max_imp <- as.numeric(quantile(study_for_imputation$confounder_1, missing_data_proportion))
#then normal distribution to add random error... AHHHHHHH
#remember min is 0 and prop_missing gives us the right quantile for calculating max from study_for_imputation
imputed_value_regular <- inv_log_quant_transform(value = imputation_value_transformed,
min = min_imp,
max = max_imp)
return(imputed_value_regular)
}
fixed_MI <- function(num_iterations = 10, missing_dataset,
basis_coefficients, var_for_imputation, #var_for_imputation is no longer used
specified_max) {
b1_coefficients <- numeric(num_iterations)
b1_SE <- numeric(num_iterations)
for(i in 1:num_iterations) {
#generate ONE uniform value
my_uniform_values <- uniform_values(specified_max = specified_max)
fixed_MI_dataset <- missing_dataset |>
mutate(confounder_1_fixed_MI = ifelse(is.na(confounder_1_missing),
imputation_algorithm(basis_coefficients = basis_coefficients,
study_for_imputation = missing_dataset,
var_for_imputation = "confounder_1_missing",
row_index = seq_len(nrow(missing_dataset)),
u_vector = my_uniform_values),
confounder_1_missing)
)
#then run logistic regression
log_reg_result <- glm(outcome ~ exposure + predictor + confounder_1_fixed_MI,
data = fixed_MI_dataset,
family = "binomial")
#store the b1 parameter in a vector
b1_coefficients[i] <- log_reg_result$coefficients[2]
b1_SE[i] <- summary(log_reg_result)$coefficients[2,2]
}
#now put the parameter estimates and their associated variances in a dataframe and return that for each MI iteration
coefficients_and_SE <- data.frame(coefficients = b1_coefficients,
SE = b1_SE)
return(coefficients_and_SE)
}
#this is the CLQI algorithm, with some modifications to the u_vector idea
#for fixed MI, just move where the my_uniform_values gets called, so we use the same uniform value each time
#sadly this is not efficient, but for readability, and also for me to not go insane, this is what I did
CLQI_algorithm <- function(missing_dataset, basis_coefficients, specified_max) {
missing_dataset$confounder_1_CLQI <- NA #initialize a new variable here... base R ugh
for(row_index in 1:nrow(missing_dataset)) {
if(is.na(missing_dataset$confounder_1_missing[row_index])) {
my_uniform_values <- uniform_values(specified_max) #generate unique uniform values
imputed_value <- imputation_algorithm(basis_coefficients = basis_coefficients,
study_for_imputation = missing_dataset,
var_for_imputation = "confounder_1_missing",
row_index = row_index, #to get the right one
u_vector = my_uniform_values)
missing_dataset$confounder_1_CLQI[row_index] <- imputed_value
}
else {
missing_dataset$confounder_1_CLQI[row_index] <- missing_dataset$confounder_1_missing[row_index]
}
}
return(missing_dataset)
}
