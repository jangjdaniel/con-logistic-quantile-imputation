if (is.na(make_missing$original_data[i])) {
quantile_value <- imputation_process(my_data) #from basis dataset
make_missing$imputed_data[i] <- quantile_value
}
else {
make_missing$imputed_data[i] <- make_missing$original_data[i]
}
}
library(tidyverse)
library(missMethods)
library(quantreg)
#initial data generation
set.seed(2025)
#skewed distribution for imputation testing
my_data <- data.frame(
original_data = rchisq(1000, df = 5)) #can change to any other distribution: try F and gamma?
#rchisq(1000, df = 5)
#rgamma(1000, 4, rate = 3)
#rf(1000, 6, 4)
#rnorm(1000, mean = 0, sd = 4)
ggplot(data = my_data, aes(x=original_data)) +
geom_density()
#the bounds for our variable: this is just a toy example
min <- 0
max <- 14 #for the sake of demonstration, let's create this cutoff
count(my_data |> filter(original_data > 14)) #as we can see, 10 of the 1000 obs are out of the range
my_data <- my_data |>
mutate(transformed_data = log((original_data - min) / (max - original_data)))
ggplot(data = my_data, aes(x=transformed_data)) +
geom_density()
#mechanism to make missing data (from https://cran.r-project.org/web/packages/missMethods/vignettes/Generating-missing-values.html)
make_missing <- delete_MCAR(my_data, 0.5, "original_data") #half the data is gone now
#likely a better way to approach this, but I'm lazy. R has gotten better with for loops
for(i in 1:nrow(make_missing)) {
if(is.na(make_missing$original_data[i])) {
make_missing$transformed_data[i] <- NA
}
}
#conditional quantile function with my_data and transformed_data!
imputation_process_logistic <- function(full_data) {
suppressWarnings({ #thanks stackexchange
unif_value <- round(runif(1, min = 0, max = 1), 2) #sample from Unif(0,1) and round to hundredths
quant_reg_coeff <- rq(transformed_data ~ 1, data = full_data, tau=unif_value) #why is this 1?
value <- coef(quant_reg_coeff)[1]
})
return(value)
}
#this time, the fitted values can only be within a certain range, so when un-transforming it, we should never observe a value outside of (min, max)
for (i in 1:nrow(make_missing)) {
if (is.na(make_missing$transformed_data[i])) {
quantile_value <- imputation_process_logistic(my_data) #from basis dataset
make_missing$imputed_data[i] <- quantile_value
}
else {
make_missing$imputed_data[i] <- make_missing$transformed_data[i]
}
}
#untransform the data
make_missing <- make_missing |>
mutate(untransformed_imputed_data = (exp(imputed_data)*max + min) / (1+exp(imputed_data)))
my_data <- my_data |>
mutate(type = "original") |>
rename(data = "original_data")
View(my_data)
my_data <- my_data |>
select(original_data) |>
mutate(type = "original") |>
rename(data = "original_data")
library(tidyverse)
library(missMethods)
library(quantreg)
#initial data generation
set.seed(2025)
#skewed distribution for imputation testing
my_data <- data.frame(
original_data = rchisq(1000, df = 5)) #can change to any other distribution: try F and gamma?
#rchisq(1000, df = 5)
#rgamma(1000, 4, rate = 3)
#rf(1000, 6, 4)
#rnorm(1000, mean = 0, sd = 4)
ggplot(data = my_data, aes(x=original_data)) +
geom_density()
#the bounds for our variable: this is just a toy example
min <- 0
max <- 14 #for the sake of demonstration, let's create this cutoff
count(my_data |> filter(original_data > 14)) #as we can see, 10 of the 1000 obs are out of the range
my_data <- my_data |>
mutate(transformed_data = log((original_data - min) / (max - original_data)))
ggplot(data = my_data, aes(x=transformed_data)) +
geom_density()
#mechanism to make missing data (from https://cran.r-project.org/web/packages/missMethods/vignettes/Generating-missing-values.html)
make_missing <- delete_MCAR(my_data, 0.5, "original_data") #half the data is gone now
#likely a better way to approach this, but I'm lazy. R has gotten better with for loops
for(i in 1:nrow(make_missing)) {
if(is.na(make_missing$original_data[i])) {
make_missing$transformed_data[i] <- NA
}
}
#conditional quantile function with my_data and transformed_data!
imputation_process_logistic <- function(full_data) {
suppressWarnings({ #thanks stackexchange
unif_value <- round(runif(1, min = 0, max = 1), 2) #sample from Unif(0,1) and round to hundredths
quant_reg_coeff <- rq(transformed_data ~ 1, data = full_data, tau=unif_value) #why is this 1?
value <- coef(quant_reg_coeff)[1]
})
return(value)
}
#this time, the fitted values can only be within a certain range, so when un-transforming it, we should never observe a value outside of (min, max)
for (i in 1:nrow(make_missing)) {
if (is.na(make_missing$transformed_data[i])) {
quantile_value <- imputation_process_logistic(my_data) #from basis dataset
make_missing$imputed_data[i] <- quantile_value
}
else {
make_missing$imputed_data[i] <- make_missing$transformed_data[i]
}
}
#untransform the data
make_missing <- make_missing |>
mutate(untransformed_imputed_data = (exp(imputed_data)*max + min) / (1+exp(imputed_data)))
my_data <- my_data |>
select(original_data) |>
mutate(type = "original") |>
rename(data = "original_data")
make_missing <- make_missing |>
select(untransformed_imputed_data) |>
mutate(type = "imputed") |>
rename(data = "imputed_data")
View(make_missing)
make_missing <- make_missing |>
select(untransformed_imputed_data) |>
mutate(type = "imputed") |>
rename(data = "untransformed_imputed_data")
#consistency
my_data <- my_data |>
select(original_data) |>
mutate(type = "original") |>
rename(data = "original_data")
#concatenating
comparison <- rbind(my_data, make_missing)
rm(my_data)
rm(make_missing)
#the plot: the imputation technique works!
ggplot(comparison, aes(data, fill = type)) +
geom_density(alpha = 0.2)
library(tidyverse)
library(missMethods)
library(quantreg)
#initial data generation
set.seed(2025)
#skewed distribution for imputation testing
my_data <- data.frame(
original_data = rchisq(1000, df = 5)) #can change to any other distribution: try F and gamma?
#rchisq(1000, df = 5)
#rgamma(1000, 4, rate = 3)
#rf(1000, 6, 4)
#rnorm(1000, mean = 0, sd = 4)
ggplot(data = my_data, aes(x=original_data)) +
geom_density()
#the bounds for our variable: this is just a toy example
min <- 0
max <- 6 #for the sake of demonstration, let's create this cutoff
count(my_data |> filter(original_data > 14)) #as we can see, 10 of the 1000 obs are out of the range
my_data <- my_data |>
mutate(transformed_data = log((original_data - min) / (max - original_data)))
ggplot(data = my_data, aes(x=transformed_data)) +
geom_density()
#mechanism to make missing data (from https://cran.r-project.org/web/packages/missMethods/vignettes/Generating-missing-values.html)
make_missing <- delete_MCAR(my_data, 0.5, "original_data") #half the data is gone now
#likely a better way to approach this, but I'm lazy. R has gotten better with for loops
for(i in 1:nrow(make_missing)) {
if(is.na(make_missing$original_data[i])) {
make_missing$transformed_data[i] <- NA
}
}
#conditional quantile function with my_data and transformed_data!
imputation_process_logistic <- function(full_data) {
suppressWarnings({ #thanks stackexchange
unif_value <- round(runif(1, min = 0, max = 1), 2) #sample from Unif(0,1) and round to hundredths
quant_reg_coeff <- rq(transformed_data ~ 1, data = full_data, tau=unif_value) #why is this 1?
value <- coef(quant_reg_coeff)[1]
})
return(value)
}
#this time, the fitted values can only be within a certain range, so when un-transforming it, we should never observe a value outside of (min, max)
for (i in 1:nrow(make_missing)) {
if (is.na(make_missing$transformed_data[i])) {
quantile_value <- imputation_process_logistic(my_data) #from basis dataset
make_missing$imputed_data[i] <- quantile_value
}
else {
make_missing$imputed_data[i] <- make_missing$transformed_data[i]
}
}
#untransform the data
make_missing <- make_missing |>
mutate(untransformed_imputed_data = (exp(imputed_data)*max + min) / (1+exp(imputed_data)))
#consistency
my_data <- my_data |>
select(original_data) |>
mutate(type = "original") |>
rename(data = "original_data")
make_missing <- make_missing |>
select(untransformed_imputed_data) |>
mutate(type = "imputed") |>
rename(data = "untransformed_imputed_data")
#concatenating
comparison <- rbind(my_data, make_missing)
rm(my_data)
rm(make_missing)
#the plot: the imputation technique works!
ggplot(comparison, aes(data, fill = type)) +
geom_density(alpha = 0.2)
#an idea that I just had
#say that we have a case where 0 to 1 is systematically missing and 1 to 14 is detected
#look at the range of a variable from the basis dataset
#calculate the quantile proportions that we need to impute and divide it by 100
#in this case, the quantiles from 0 to 1 is 0.01 to 0.06. generate 99 equidistant quantiles here
#then using the basis dataset, impute all missing values from 0 to 1 using these quantiles
#profit?
#did I miss something?
library(tidyverse)
library(missMethods)
library(quantreg)
#initial data generation
set.seed(2025)
#skewed distribution for imputation testing
my_data <- data.frame(
original_data = rchisq(1000, df = 5)) #can change to any other distribution: try F and gamma?
#rchisq(1000, df = 5)
#rgamma(1000, 4, rate = 3)
#rf(1000, 6, 4)
#rnorm(1000, mean = 0, sd = 4)
ggplot(data = my_data, aes(x=original_data)) +
geom_density()
#the bounds for our variable: this is just a toy example
min <- 0
max <- 14 #for the sake of demonstration, let's create this cutoff
count(my_data |> filter(original_data > 14)) #as we can see, 10 of the 1000 obs are out of the range
my_data <- my_data |>
mutate(transformed_data = log((original_data - min) / (max - original_data)))
ggplot(data = my_data, aes(x=transformed_data)) +
geom_density()
#mechanism to make missing data (from https://cran.r-project.org/web/packages/missMethods/vignettes/Generating-missing-values.html)
make_missing <- delete_MCAR(my_data, 0.5, "original_data") #half the data is gone now
#likely a better way to approach this, but I'm lazy. R has gotten better with for loops
for(i in 1:nrow(make_missing)) {
if(is.na(make_missing$original_data[i])) {
make_missing$transformed_data[i] <- NA
}
}
#conditional quantile function with my_data and transformed_data!
imputation_process_logistic <- function(full_data) {
suppressWarnings({ #thanks stackexchange
unif_value <- round(runif(1, min = 0, max = 1), 2) #sample from Unif(0,1) and round to hundredths
quant_reg_coeff <- rq(transformed_data ~ 1, data = full_data, tau=unif_value) #why is this 1?
value <- coef(quant_reg_coeff)[1]
})
return(value)
}
#this time, the fitted values can only be within a certain range, so when un-transforming it, we should never observe a value outside of (min, max)
for (i in 1:nrow(make_missing)) {
if (is.na(make_missing$transformed_data[i])) {
quantile_value <- imputation_process_logistic(my_data) #from basis dataset
make_missing$imputed_data[i] <- quantile_value
}
else {
make_missing$imputed_data[i] <- make_missing$transformed_data[i]
}
}
#untransform the data
make_missing <- make_missing |>
mutate(untransformed_imputed_data = (exp(imputed_data)*max + min) / (1+exp(imputed_data)))
#consistency
my_data <- my_data |>
select(original_data) |>
mutate(type = "original") |>
rename(data = "original_data")
make_missing <- make_missing |>
select(untransformed_imputed_data) |>
mutate(type = "imputed") |>
rename(data = "untransformed_imputed_data")
#concatenating
comparison <- rbind(my_data, make_missing)
rm(my_data)
rm(make_missing)
#the plot: the imputation technique works!
ggplot(comparison, aes(data, fill = type)) +
geom_density(alpha = 0.2)
#quick note: changing max to a value like 6 shows that the limited range works!
#it does show that we need to be very careful in choosing our min and max values
#an idea that I just had
#say that we have a case where 0 to 1 is systematically missing and 1 to 14 is detected
#look at the range of a variable from the basis dataset
#calculate the quantile proportions that we need to impute and divide it by 100
#in this case, the quantiles from 0 to 1 is 0.01 to 0.06. generate 99 equidistant quantiles here
#then using the basis dataset, impute all missing values from 0 to 1 using these quantiles
#profit?
#did I miss something?
test_data <- data.frame(
original_data = rchisq(1000, df = 5)) #can change to any other distribution: try F and gamma?
View(test_data)
test_data <- test_data |>
mutate(limited_data = original_data) |>
filter(limited_data > 1)
test_data <- test_data |>
mutate(limited_data = original_data) |>
filter(limited_data >= 2)
test_data <- data.frame(
original_data = rchisq(1000, df = 5)) #can change to any other distribution: try F and gamma?
test_data <- test_data |>
mutate(limited_data = ifelse(original_data > 1, original_data, NA))
ggplot(test_data, aes(original_data, limited_data)) +
geom_density(alpha = 0.2)
ggplot(test_data, aes(x=original_data, y= limited_data)) +
geom_density(alpha = 0.2)
ggplot(test_data, aes(x=original_data, y= limited_data))
ggplot(test_data, aes(x=original_data, y= limited_data)) +
geom_point())
ggplot(test_data, aes(x=original_data, y= limited_data)) +
geom_point()
14-0
#some generalized calculations
1/(14-0)
#some generalized calculations
1/(max-min) #each partition's size
#some generalized calculations
1/(max-min)*100 #each partition's size
1/100 * (max-min)
1/100 * (14-6)
1/99 * (max-min) #the distance of each quantile
left_limit - min #the distance of the limit of detection
#limit of detection problems
left_limit <- 1
left_limit - min #the distance of the limit of detection
test_data <- test_data |>
mutate(left_basis = ifelse(original_data <= left_limit), original_data, NA)
test_data <- test_data |>
mutate(left_basis = ifelse(original_data <= left_limit, original_data, NA))
hist(test_data$left_limit)
hist(test_data$left_basis)
count(test_data$left_basis)
tally(test_data$left_basis)
nrow(test_data$left_basis)
ncol(test_data$left_basis)
(test_data$left_basis)
length(test_data$left_basis)
length(test_data$left_basis, na.rm=TRUE)
sum(!is.na(test_data$left_basis))
for (i in seq(0, 1, by = 0.01)) {
quant_reg_coeff <- rq(left_basis ~ 1, data = test_Data, tau=i) #why is this 1?
value <- coef(quant_reg_coeff)[1]
return(value)
}
quant_reg_coeff <- rq(left_basis ~ 1, data = test_data, tau=i) #why is this 1?
for (i in seq(0, 1, by = 0.01)) {
quant_reg_coeff <- rq(left_basis ~ 1, data = test_data, tau=i) #why is this 1?
value <- coef(quant_reg_coeff)[1]
return(value)
}
for (i in seq(0, 1, by = 0.01)) {
quant_reg_coeff <- rq(left_basis ~ 1, data = test_data, tau=i) #why is this 1?
value <- coef(quant_reg_coeff)[1]
vect[i] <- value
return(value)
}
vect <- c()
for (i in seq(0, 1, by = 0.01)) {
quant_reg_coeff <- rq(left_basis ~ 1, data = test_data, tau=i) #why is this 1?
value <- coef(quant_reg_coeff)[1]
vect[i] <- value
return(value)
}
quant_reg_coeff <- rq(left_basis ~ 1, data = test_data, tau=i) #why is this 1?
value <- coef(quant_reg_coeff)[1]
vect[i] <- value
vect[i*100] <- value
for (i in seq(0, 1, by = 0.01)) {
quant_reg_coeff <- rq(left_basis ~ 1, data = test_data, tau=i) #why is this 1?
value <- coef(quant_reg_coeff)[1]
vect[i*100] <- value
return(value)
}
vect <- c()
index <- 1  # Initialize index
for (i in seq(0, 1, by = 0.01)) {
quant_reg_coeff <- rq(left_basis ~ 1, data = test_data, tau = i)
value <- coef(quant_reg_coeff)[1]
vect[index] <- value  # Use the index for storing the value
index <- index + 1    # Increment the index for the next iteration
}
vect()
hist(vect)
test_data <- data.frame(
original_data = rchisq(1000, df = 1)) #can change to any other distribution: try F and gamma?
test_data <- test_data |>
mutate(limited_data = ifelse(original_data > 1, original_data, NA)) #anything lower than 1 is removed
#some generalized calculations
left_lim_dist <- left_limit - min #the distance of the limit of detection
right_lim_dist <- max - right_limit
test_data <- test_data |>
mutate(left_basis = ifelse(original_data <= left_limit, original_data, NA))
right_lim_dist <- max - right_limit
right_limit <- 14
hist(test_data$left_basis)
sum(!is.na(test_data$left_basis)) #sample of 42... is that enough?? let's see
sum(!is.na(test_data$left_basis)) #sample of 660... a lot!
vect <- c()
index <- 1  # Initialize index
for (i in seq(0, 1, by = 0.01)) {
quant_reg_coeff <- rq(left_basis ~ 1, data = test_data, tau = i)
value <- coef(quant_reg_coeff)[1]
vect[index] <- value  # Use the index for storing the value
index <- index + 1    # Increment the index for the next iteration
}
imputation_process_limited <- function(limited_data) {
imputation_process_limited <- function(limited_data) {
suppressWarnings({ #thanks stackexchange
unif_value <- round(runif(1, min = 0, max = 1), 2) #sample from Unif(0,1) and round to hundredths
quant_reg_coeff <- rq(left_basis ~ 1, data = limited_data, tau=unif_value) #why is this 1?
value <- coef(quant_reg_coeff)[1]
})
}
for (i in 1:nrow(test_data)) {
if (is.na(test_data$limited_data[i])) {
quantile_value <- imputation_process_limited(test_data) #from basis dataset
test_data$limited_data[i] <- quantile_value
}
else {
test_data$limited_data[i] <- test_data$limited_data[i]
}
}
imputation_process_limited <- function(limited_data) {
suppressWarnings({ #thanks stackexchange
unif_value <- round(runif(1, min = 0, max = 1), 2) #sample from Unif(0,1) and round to hundredths
quant_reg_coeff <- rq(left_basis ~ 1, data = limited_data, tau=unif_value) #why is this 1?
value <- coef(quant_reg_coeff)[1]
})
}
for (i in 1:nrow(test_data)) {
if (is.na(test_data$limited_data[i])) {
quantile_value <- imputation_process_limited(test_data) #from basis dataset
test_data$limited_data[i] <- quantile_value
}
else {
test_data$limited_data[i] <- test_data$limited_data[i]
}
}
View(test_data)
View(test_data)
View(test_data)
View(test_data)
View(test_data)
View(test_data)
library(tidyverse)
library(missMethods)
library(quantreg)
#initial data generation
set.seed(2025)
#skewed distribution for imputation testing
my_data <- data.frame(
original_data = rchisq(1000, df = 5)) #can change to any other distribution: try F and gamma?
#rchisq(1000, df = 5)
#rgamma(1000, 4, rate = 3)
#rf(1000, 6, 4)
#rnorm(1000, mean = 0, sd = 4)
ggplot(data = my_data, aes(x=original_data)) +
geom_density()
#playing around with some ideas:
min <- 0
max <- 14
#limit of detection problems
left_limit <- 1
right_limit <- 14
#say we had code to show that from 0 to 1 we cannot detect
test_data <- data.frame(
original_data = rchisq(1000, df = 1)) #can change to any other distribution: try F and gamma?
test_data <- test_data |>
mutate(limited_data = ifelse(original_data > 1, original_data, NA)) #anything lower than 1 is removed
#some generalized calculations
left_lim_dist <- left_limit - min #the distance of the limit of detection
right_lim_dist <- max - right_limit
#from here, you create a subset of the dataset in this manner
test_data <- test_data |>
mutate(left_basis = ifelse(original_data <= left_limit, original_data, NA))
hist(test_data$left_basis)
sum(!is.na(test_data$left_basis)) #sample of 660... a lot!
imputation_process_limited <- function(limited_data) {
suppressWarnings({ #thanks stackexchange
unif_value <- round(runif(1, min = 0, max = 1), 2) #sample from Unif(0,1) and round to hundredths
quant_reg_coeff <- rq(left_basis ~ 1, data = limited_data, tau=unif_value) #why is this 1?
value <- coef(quant_reg_coeff)[1]
})
}
for (i in 1:nrow(test_data)) {
if (is.na(test_data$limited_data[i])) {
quantile_value <- imputation_process_limited(test_data) #from basis dataset
test_data$limited_data[i] <- quantile_value
}
else {
test_data$limited_data[i] <- test_data$limited_data[i]
}
}
for (i in 1:nrow(test_data)) {
if (is.na(test_data$limited_data[i])) {
quantile_value <- imputation_process_limited(test_data) #from basis dataset
test_data$limited_data[i] <- quantile_value
}
else {
test_data$limited_data[i] <- test_data$limited_data[i]
}
}
