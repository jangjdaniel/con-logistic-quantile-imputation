#run logistic regression
log_reg_result <- glm(outcome ~ exposure + predictor + confounder_1_CLQI,
data = missing_dataset_imputed,
family = "binomial")
#extract exposure coefficients (b1)
b1_coefficients[i] <- log_reg_result$coefficients[2]
b1_SE[i] <- summary(log_reg_result)$coefficients[2,2]
}
#now make a dataframe for the coefficients and variance for each MI iteration
coefficients_and_SE <- data.frame(coefficients = b1_coefficients,
SE = b1_SE)
return(coefficients_and_SE)
}
constant_imputation <- function(missing_dataset, var_for_imputation, LD) {
missing_dataset_constanted <- missing_dataset |>
dplyr::mutate(confounder_1_sqrt2_imp = ifelse(is.na(confounder_1_missing),
(LD / sqrt(2)),
confounder_1_missing))
log_reg_result <- glm(outcome ~ exposure + predictor + confounder_1_sqrt2_imp,
data = missing_dataset_constanted,
family = "binomial")
constant_imp_result <- c(log_reg_result$coefficients[2], summary(log_reg_result)$coefficients[2,2])
return(constant_imp_result) #for the exposure variable
}
complete_case_analysis <- function(missing_dataset, var_for_imputation) {
CC_missing_dataset <- missing_dataset |>
filter(!is.na(confounder_1_missing)) #only keep the values where confounder_1_missing is still observed
log_reg_result <- glm(outcome ~ exposure + predictor + confounder_1_missing,
data = CC_missing_dataset,
family = "binomial")
CC_results <- c(log_reg_result$coefficients[2], summary(log_reg_result)$coefficients[2,2])
return(CC_results) #for the exposure variable
}
rubin_rules <- function(estimate_and_variance) {
#the parameter result
coefficient_MI <- mean(estimate_and_variance$coefficients)
#the SE result: google rubin's rules it's quite standard
variance_within_MI <- mean((estimate_and_variance$SE)^2)
variance_between_MI <- sum((estimate_and_variance$coefficients - coefficient_MI)^2) / (length(estimate_and_variance$coefficients) - 1)
total_variance <- variance_within_MI + variance_between_MI + (variance_between_MI / length(estimate_and_variance$coefficients))
SE_MI <- sqrt(total_variance)
#return a vector, where [1] is the estimate and [2] is the variance
my_vector <- c(coefficient_MI, SE_MI)
return(my_vector)
}
set.seed(605) #set seed for reproducibility
my_sample_size = 500
my_tau = 0.01
one_iteration <- function(specified_sample_size, specified_tau, specified_LD) {
#generate the data
#create the data here so we can do the same imputation with the same data
my_data <- data_generating_mechanism(sample_size = specified_sample_size,
LD = specified_LD,
tau = specified_tau)
#our studies!
basis_study <- as.data.frame(my_data[[1]])
missing_study <- as.data.frame(my_data[[2]])
the_LD <- min(missing_study$confounder_1_missing, na.rm=TRUE)
#CLQI
#the basis dataset runs conditional logistic quantile regression
basis_regression_coefficents <- logistic_quantile_regression_coefficients(basis_study) #created from my_data
prop <- find_prop_missing(missing_study, "confounder_1_missing")
#the CLQI_MI results
CLQI_MI_results <- CLQI_MI(num_iterations = 10,
missing_dataset = missing_study, #created from my_data
basis_coefficients = basis_regression_coefficents,
var_for_imputation = "confounder_1_missing", #again not used
specified_max = prop)
#combine these results using rubin's rules (since the function returns the 10 iterations from MI)
CLQI_MI_results <- rubin_rules(CLQI_MI_results)
#the fixed MI results
fixed_MI_results <- fixed_MI(num_iterations = 10,
missing_dataset = missing_study,
basis_coefficients = basis_regression_coefficents,
specified_max = prop)
#combine these results using rubin's rules (since the function returns the 10 iterations from MI)
fixed_MI_results <- rubin_rules(fixed_MI_results)
###############################################################################################
#the constant imputation results
constant_imputation_sqrt_2_results <- constant_imputation(missing_dataset = missing_study,
LD = the_LD)
#the complete case results
complete_case_results <- complete_case_analysis(missing_dataset = missing_study)
###############################################################################################
#turning this into a vector: the specific order of our methods will matter
imputation_results_vector <- data.frame(estimate_CLQI = CLQI_MI_results[1],
SE_CLQI = CLQI_MI_results[2],
estimate_fixed_MI = fixed_MI_results[1],
SE_fixed_MI = fixed_MI_results[2],
estimate_constant = constant_imputation_sqrt_2_results[1],
SE_constant = constant_imputation_sqrt_2_results[2],
estimate_CC = complete_case_results[1],
SE_CC = complete_case_results[2])
return(imputation_results_vector)
}
repetera_full_simulation <- function(specified_sample_size, specified_tau, specified_LD, sim_size, b1_true) {
sim_results <- purrr::map_dfr(1:sim_size, ~ {
# We go through one iteration of the simulation
parameter_est <- one_iteration(specified_sample_size = specified_sample_size,
specified_tau = specified_tau,
specified_LD = specified_LD)
# Return the result of the current iteration
parameter_est
})
#now for coverage: need to calculate based on a condition
t_star <- qt(0.975, df = specified_sample_size - 1)
#after, we will calculate the statistics we desire... STILL HAVE NOT DONE COVERAGE YET
sim_results_send <- sim_results |>
mutate(bias_CLQI = estimate_CLQI - b1_true,
MSE_CLQI = (bias_CLQI)^2 + (SE_CLQI)^2,
coverage_CLQI = ifelse(estimate_CLQI - (t_star*SE_CLQI) <= b1_true & b1_true <= estimate_CLQI + (t_star*SE_CLQI), 1, 0),
bias_fixed_MI = estimate_fixed_MI - b1_true,
MSE_fixed_MI = (bias_fixed_MI)^2 + (SE_fixed_MI)^2,
coverage_fixed_MI = ifelse(estimate_fixed_MI - (t_star*SE_fixed_MI) <= b1_true & b1_true <= estimate_fixed_MI + (t_star*SE_fixed_MI), 1, 0),
bias_constant = estimate_constant - b1_true,
MSE_constant = (bias_constant)^2 + (SE_constant)^2,
coverage_constant = ifelse(estimate_constant - (t_star*SE_constant) <= b1_true & b1_true <= estimate_constant + (t_star*SE_constant), 1, 0),
bias_CC = estimate_CC - b1_true,
MSE_CC = (bias_CC)^2 + (SE_CC)^2,
coverage_CC = ifelse(estimate_CC - (t_star*SE_CC) <= b1_true & b1_true <= estimate_CC + (t_star*SE_CC), 1, 0)
)
return(sim_results_send)
}
tictoc::tic() #start the timer to check
sim_setting_1_results <- repetera_full_simulation(specified_sample_size = 500,
specified_tau = 0,
specified_LD = 0.50, #50% missing data
sim_size = 1000,
b1_true = log(1.5))
tictoc::toc() # Finish the computation time
#save the results and look at them!
saveRDS(sim_setting_1_results, file="./simulation_results/sim_setting_1.rds")
sim_setting_1_results <- readRDS(file="./simulation_results/sim_setting_1.rds")
ggplot(data=sim_setting_1_results, aes(x=bias_CLQI)) +
geom_density() +
geom_vline(aes(xintercept = mean(bias_CLQI)), color = "red", linetype = "dashed", size = 1) +
theme_bw()
#the bias... hmm
mean(sim_setting_1_results$bias_CLQI)
sim_setting_1_results_bias <- sim_setting_1_results |>
pivot_longer(cols = c(bias_fixed_MI, bias_CLQI, bias_constant, bias_CC),
names_to = "bias_type",
values_to = "bias_value")
# Compute the means of bias_value for each bias_type
mean_bias_values <- sim_setting_1_results_bias |>
group_by(bias_type) |>
summarize(mean_bias = mean(bias_value))
# Plot the density of each bias type with separate graphs
ggplot(sim_setting_1_results_bias, aes(x = bias_value, fill = bias_type)) +
geom_density(alpha = 0.25) +  # Adjust transparency with alpha
geom_vline(data = mean_bias_values,
aes(xintercept = mean_bias, color = bias_type),
linetype = "dashed", size = 1) +
theme_bw() +
labs(title = "Density Plots of Bias Types", x = "Bias Value", y = "Density") +
scale_fill_manual(values = c("blue", "red", "green", "purple")) +  # Adjust colors as desired
scale_color_manual(values = c("blue", "red", "green", "purple")) +  # Same color scheme for the lines
facet_wrap(~bias_type, scales = "free")  # Separate graphs for each bias type
sim_setting_1_results_MSE <- sim_setting_1_results |>
pivot_longer(cols = c(MSE_fixed_MI, MSE_CLQI, MSE_constant, MSE_CC),
names_to = "MSE_type",
values_to = "MSE_value")
# Plot the density of each bias type
ggplot(sim_setting_1_results_MSE, aes(x = MSE_value, fill = MSE_type)) +
geom_density(alpha = 0.25) +  # Adjust transparency with alpha
geom_vline(data = sim_setting_1_results_MSE |>
group_by(MSE_type) |>
summarize(mean_MSE = mean(MSE_value)),
aes(xintercept = mean_MSE, color = MSE_type),
linetype = "dashed", size = 1) +
theme_bw() +
labs(title = "Density Plots of MSE Types", x = "MSE Value", y = "Density") +
scale_fill_manual(values = c("blue", "red", "green", "purple")) +  # Adjust colors as desired
scale_color_manual(values = c("blue", "red", "green", "purple")) + # Same color scheme for the lines
facet_wrap(~MSE_type, scales = "free")  +
xlim(0,0.5)
sim_setting_1_coverage <- sim_setting_1_results |>
summarize(cov_perc_CLQI = sum(coverage_CLQI) / nrow(sim_setting_1_results),
cov_perc_fixed_MI = sum(coverage_fixed_MI) / nrow(sim_setting_1_results),
cov_perc_constant = sum(coverage_constant) / nrow(sim_setting_1_results),
cov_perc_CC = sum(coverage_CC) / nrow(sim_setting_1_results))
sim_setting_1_coverage
sim_setting_1_coverage <- sim_setting_1_results |>
summarize(cov_perc_CLQI = sum(coverage_CLQI) / nrow(sim_setting_1_results),
cov_perc_fixed_MI = sum(coverage_fixed_MI) / nrow(sim_setting_1_results),
cov_perc_constant = sum(coverage_constant) / nrow(sim_setting_1_results),
cov_perc_CC = sum(coverage_CC) / nrow(sim_setting_1_results))
sim_setting_1_coverage
#necessary libraries
library(tidyverse)
library(ggplot2)
library(quantreg)
library(MASS)
library(purrr)
library(tictoc)
#logit and expit functions for myself
logit <- function(prob) {
value <- log(prob / (1 - prob))
return(value)
}
expit <- function(x) {
value <- 1 / (1 + exp(-x))
return(value)
}
#now some other transformation functions
log_quant_transform <- function(value, min, max) {
new_value <- log((value - min) / (max - value))
if (is.nan(new_value)) {return(NA)} #negative log values return an NA
else {return(new_value)}
}
inv_log_quant_transform <- function(value, min, max) {
new_value <- (exp(value)*max + min) / (1+exp(value))
return(new_value)
}
logit(expit(3))
test_case_chi_sqaure <- rchisq(n = 1000, df = 1)
test_case_chi_sqaure
test_case_chi_sqaure <- rchisq(n = 1000, df = 5)
test_case_chi_sqaure <- rchisq(n = 1000, df = 5)
median(test_case_chi_sqaure)
log_quant_transform(value = median(test_case_chi_sqaure),
min = min(test_case_chi_sqaure),
max = max(test_case_chi_sqaure))
logit(expit(3)) #should return 3 as logit is the inverse function of expit
test_case_chi_sqaure <- rchisq(n = 1000, df = 5)
#we will transform the median value
median_value_transformed <- log_quant_transform(value = median(test_case_chi_sqaure),
min = min(test_case_chi_sqaure),
max = max(test_case_chi_sqaure))
#then untransform this value
median_value_untransformed <- inv_log_quant_transform(value = median_value_transformed,
min = min(test_case_chi_sqaure),
max = max(test_case_chi_sqaure))
identical(median(test_case_chi_sqaure), median_value_untransformed)
median_value_untransformed
median(test_case_chi_sqaure)
median_value_untransformed
identical(median(test_case_chi_sqaure), median_value_untransformed)
median(test_case_chi_sqaure)
median_value_untransformed
error_allowed <- 0.0000001
if(abs(median(test_case_chi_sqaure) - median_value_untransformed) > error_allowed) {
print("False")
}
if(abs(median(test_case_chi_sqaure) - median_value_untransformed) > error_allowed) {
print("False")
} else {print("True")}
logit(expit(3)) #should return 3 as logit is the inverse function of expit
test_case_chi_sqaure <- rchisq(n = 1000, df = 5)
#we will transform the median value
median_value_transformed <- log_quant_transform(value = median(test_case_chi_sqaure),
min = min(test_case_chi_sqaure),
max = max(test_case_chi_sqaure))
#then untransform this value
median_value_untransformed <- inv_log_quant_transform(value = median_value_transformed,
min = min(test_case_chi_sqaure),
max = max(test_case_chi_sqaure))
#to avoid a floating point issue, use this code to confirm that the functions are truly inverses of one another
error_allowed <- 0.0000001
difference <- median(test_case_chi_sqaure) - median_value_untransformed
if(abs(difference) > error_allowed) {
print("Check your function again")
} else {print("Function is working as intended")}
coefficient_generator(tau = 0)
coefficient_generator <- function(tau, num_studies = 2, #we are just doing the case where we have two studies in consideration
a_0 = logit(0.3), a_1 = log(1.1),
b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2)) {
#create which number study we have
study_counts <- data.frame(id = 1:num_studies)
study_counts <- study_counts |>
mutate(study_num = paste("Study", id)) |>
dplyr::select(study_num)
#allow a_1 to vary
alpha_coefficients <- rnorm(num_studies, a_1, tau)
alpha_coefficients <- as.data.frame(alpha_coefficients) #make into data frame
#renaming for better binding experience + adding a_0... not important here
alpha_coefficients <- alpha_coefficients |>
mutate(a_0 = a_0) |>
rename(a_1 = "alpha_coefficients") |>
dplyr::select(a_0, a_1) #reordering
#now doing the beta coefficients
beta_vector <- c(b_1, b_2, b_3)
#variance-covariance matrix
matrix_size <- length(beta_vector)
diag_mat <- matrix(0, matrix_size, matrix_size)
diag(diag_mat) <- 1 #make all the diagonals 1 for the identity matrix
#lastly, we need to perform the calculation specified in section 3.1.4
beta_matrix <- tau * diag_mat
beta_coefficients <- mvrnorm(num_studies, beta_vector, beta_matrix)
beta_coefficients <- as.data.frame(beta_coefficients)
#renaming for better binding experience
beta_coefficients <- beta_coefficients |>
mutate(b_0 = b_0) |>
rename(b_1 = "V1", b_2 = "V2", b_3 = "V3") |>
dplyr::select(b_0, b_1, b_2, b_3)
#lastly, we need a range of degrees of freedom for the confounder generation
#there will be two just in case I want to use an F distribution
if(tau == 0) { #common
deg_freedom <- data.frame(df_1 = round(runif(num_studies, min = 5, max = 5)), #don't touch it if it aint broke
df_2 = round(runif(num_studies, min = 5, max = 5)))
} else { #heterogenous
deg_freedom <- data.frame(df_1 = round(runif(num_studies, min = 3, max = 7)),
df_2 = round(runif(num_studies, min = 3, max = 7)))
}
#now combine these results
coefficients <- cbind(study_counts, alpha_coefficients, beta_coefficients, deg_freedom)
return(coefficients)
}
coefficient_generator(tau = 0)
#step 1: Base Binary Predictor
V <- rbinom(n=1000, size = 1, prob = 0.4)
V
mean(V)
create_data(n=5000)
create_data <- function(sample_size,
a_0 = logit(0.3), a_1 = log(1.1),
b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2),
df_1, df_2) { #maybe remove df_2
#initialize everything
n <- sample_size
#step 1: Base Binary Predictor
V <- rbinom(n, size = 1, prob = 0.4)
#step 2: confounder with a skewed distribution
C1 <- rchisq(n, df = df_1)
#step 3: generating exposure variable based on confounders (probability)
E <- plogis(a_0 + a_1*C1)
E_bin <- rbinom(n, size = 1, prob = E) #USE THIS IN NEXT REGRESSION!!
#step 4: generating outcome based on confounders, exposure, and base binary predictor
O <- plogis(b_0 + b_1*E_bin + b_2*V + b_3*C1)
O_bin <- rbinom(n, size = 1, prob = O)
#step 5: create dataset
my_data <- data.frame(
predictor = V,
confounder_1 = C1,
exposure = E_bin,
outcome = O_bin
)
return(my_data)
}
create_data(n=5000)
create_data(sample_size = 5000)
create_data(sample_size = 5000, df_1 = 5)
glm(outcome ~ exposure + confounder_1 + predictor,
data = test_data_create,
family = "binomial")
test_data_create <- create_data(sample_size = 5000, df_1 = 5)
glm(outcome ~ exposure + confounder_1 + predictor,
data = test_data_create,
family = "binomial")
test_log_reg <- glm(outcome ~ exposure + confounder_1 + predictor,
data = test_data_create,
family = "binomial")
summary(test_log_reg)
test_data_create <- create_data(sample_size = 25000, df_1 = 5)
test_log_reg <- glm(outcome ~ exposure + confounder_1 + predictor,
data = test_data_create,
family = "binomial")
summary(test_log_reg)
test_data_create <- create_data(sample_size = 25000, df_1 = 5)
test_log_reg <- glm(outcome ~ exposure + confounder_1 + predictor,
data = test_data_create,
family = "binomial")
summary(test_log_reg)
test_data_create <- create_data(sample_size = 50000, df_1 = 5)
test_log_reg <- glm(outcome ~ exposure + confounder_1 + predictor,
data = test_data_create,
family = "binomial")
summary(test_log_reg)
test_data_create <- create_data(sample_size = 50000, df_1 = 5)
test_log_reg <- glm(outcome ~ exposure + confounder_1 + predictor,
data = test_data_create,
family = "binomial")
summary(test_log_reg)
test_data_create <- create_data(sample_size = 50000, df_1 = 5)
test_log_reg <- glm(outcome ~ exposure + confounder_1 + predictor,
data = test_data_create,
family = "binomial")
summary(test_log_reg)
#b0 to b3
logit(0.1)
log(1.5)
log(0.7)
log(1.2)
test_data_create <- create_data(sample_size = 50000, df_1 = 5)
test_log_reg <- glm(outcome ~ exposure + predictor + confounder_1,
data = test_data_create,
family = "binomial")
summary(test_log_reg)
#match b0 to b3... should be quite close
logit(0.1)
log(1.5)
log(0.7)
log(1.2)
create_data <- function(sample_size,
a_0, a_1, b_0, b_1, b_2, b_3,
df_1, df_2) { #maybe remove df_2
#initialize everything
n <- sample_size
#step 1: Base Binary Predictor
V <- rbinom(n, size = 1, prob = 0.4)
#step 2: confounder with a skewed distribution
C1 <- rchisq(n, df = df_1)
#step 3: generating exposure variable based on confounders (probability)
E <- plogis(a_0 + a_1*C1)
E_bin <- rbinom(n, size = 1, prob = E) #USE THIS IN NEXT REGRESSION!!
#step 4: generating outcome based on confounders, exposure, and base binary predictor
O <- plogis(b_0 + b_1*E_bin + b_2*V + b_3*C1)
O_bin <- rbinom(n, size = 1, prob = O)
#step 5: create dataset
my_data <- data.frame(
predictor = V,
confounder_1 = C1,
exposure = E_bin,
outcome = O_bin
)
return(my_data)
}
test_data_create <- create_data(sample_size = 50000,
a_0 = logit(0.3), a_1 = log(1.1),
b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2),
df_1 = 5)
test_log_reg <- glm(outcome ~ exposure + predictor + confounder_1,
data = test_data_create,
family = "binomial")
summary(test_log_reg)
#match b0 to b3... should be quite close
logit(0.1)
log(1.5)
log(0.7)
log(1.2)
test_coefficients <- coefficient_generator(tau = 0)
test_coefficients <- coefficient_generator(tau = 0)
create_multiple_datasets(study_coefficient_dataset = test_coefficients,
sample_size = 50000)
#recall all coefficients are stored in coefficient_generator funct
create_multiple_datasets <- function(study_coefficient_dataset, sample_size) { #requires coefficients to be given
my_list <- list() #initialize empty list
#pulling all values we need
a_0_values <- study_coefficient_dataset |> dplyr::pull(a_0)
a_1_values <- study_coefficient_dataset |> dplyr::pull(a_1)
b_0_values <- study_coefficient_dataset |> dplyr::pull(b_0)
b_1_values <- study_coefficient_dataset |> dplyr::pull(b_1)
b_2_values <- study_coefficient_dataset |> dplyr::pull(b_2)
b_3_values <- study_coefficient_dataset |> dplyr::pull(b_3)
#also degrees of freedom
df_1_values <- study_coefficient_dataset |> dplyr::pull(df_1)
for(i in 1:nrow(study_coefficient_dataset)) {
#do the data generating mechanism
a_0 <- a_0_values[i]
a_1 <- a_1_values[i]
b_0 <- b_0_values[i]
b_1 <- b_1_values[i]
b_2 <- b_2_values[i]
b_3 <- b_3_values[i]
#also the degrees of freedom
df_1 <- df_1_values[i]
#apply the data generating mechanism function from the values above
my_data <- create_data(sample_size = sample_size,
a_0 = a_0,
a_1 = a_1,
b_0 = b_0,
b_1 = b_1,
b_2 = b_2,
b_3 = b_3,
df_1 = df_1)
#finally, add this dataset to our list
my_list[[i]] <- my_data
}
#return the list
return(my_list)
}
test_coefficients <- coefficient_generator(tau = 0)
create_multiple_datasets(study_coefficient_dataset = test_coefficients,
sample_size = 50000)
test_coefficients <- coefficient_generator(tau = 0)
test_data_create <- create_multiple_datasets(study_coefficient_dataset = test_coefficients,
sample_size = 50000)
#similar to before, check the logistic regressions for each dataset
test_log_reg_1 <- glm(outcome ~ exposure + predictor + confounder_1,
data = as.data.frame(test_data_create[[1]]),
family = "binomial")
test_log_reg_2 <- glm(outcome ~ exposure + predictor + confounder_1,
data = as.data.frame(test_data_create[[2]]),
family = "binomial")
summary(test_log_reg_1)
summary(test_log_reg_2)
#match b0 to b3... should be quite close
logit(0.1)
log(1.5)
log(0.7)
log(1.2)
test_coefficients <- coefficient_generator(tau = 0)
test_data_create <- create_multiple_datasets(study_coefficient_dataset = test_coefficients,
sample_size = 50000)
#similar to before, check the logistic regressions for each dataset
test_log_reg_1 <- glm(outcome ~ exposure + predictor + confounder_1,
data = as.data.frame(test_data_create[[1]]),
family = "binomial")
test_log_reg_2 <- glm(outcome ~ exposure + predictor + confounder_1,
data = as.data.frame(test_data_create[[2]]),
family = "binomial")
summary(test_log_reg_1)
summary(test_log_reg_2)
#match b0 to b3... should be quite close
logit(0.1)
log(1.5)
log(0.7)
log(1.2)
#test case
jokes_are_funny <- data_generating_mechanism(sample_size = 2500,
LD = 0.50,
tau = 0)
