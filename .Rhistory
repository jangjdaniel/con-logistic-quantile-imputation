basis_study <- as.data.frame(jokes_are_funny[[1]])
missing_study <- as.data.frame(jokes_are_funny[[2]])
the_LD <- min(missing_study$confounder_1_missing, na.rm=TRUE)
#create basis coefficients
basis_regression_coefficents <- logistic_quantile_regression_coefficients(basis_study) #created from sim_data
prop <- find_prop_missing(missing_study, "confounder_1_missing")
prop
prop
missing_study
#test case!
jokes_are_funny <- data_generating_mechanism(sample_size = 500,
LD = 0.30, #placeholder
tau = 0.01)
#our studies!
basis_study <- as.data.frame(jokes_are_funny[[1]])
missing_study <- as.data.frame(jokes_are_funny[[2]])
the_LD <- min(missing_study$confounder_1_missing, na.rm=TRUE)
#create basis coefficients
basis_regression_coefficents <- logistic_quantile_regression_coefficients(basis_study) #created from sim_data
prop <- find_prop_missing(missing_study, "confounder_1_missing")
fixed_MI_results <- fixed_MI(missing_dataset = missing_study,
basis_coefficients = basis_regression_coefficents,
specified_max = prop)
mean(fixed_MI_results) - log(1.1) #here is our bias!
prop
#test case 2
jokes_are_funny <- data_generating_mechanism(sample_size = 500,
LD = 0.30, #placeholder
tau = 0.01)
#our studies!
basis_study <- as.data.frame(jokes_are_funny[[1]])
missing_study <- as.data.frame(jokes_are_funny[[2]])
the_LD <- min(missing_study$confounder_1_missing, na.rm=TRUE)
#create basis coefficients
basis_regression_coefficents <- logistic_quantile_regression_coefficients(basis_study) #created from sim_data
prop <- find_prop_missing(missing_study, "confounder_1_missing")
fixed_MI_results <- fixed_MI(missing_dataset = missing_study,
basis_coefficients = basis_regression_coefficents,
specified_max = prop)
mean(fixed_MI_results) - log(1.1) #here is our bias!
fixed_MI_results
#test case!
jokes_are_funny <- data_generating_mechanism(sample_size = 500,
LD = 0.30,
tau = 0.01)
#our studies!
basis_study <- as.data.frame(jokes_are_funny[[1]])
missing_study <- as.data.frame(jokes_are_funny[[2]])
the_LD <- min(missing_study$confounder_1_missing, na.rm=TRUE)
#create basis coefficients
basis_regression_coefficents <- logistic_quantile_regression_coefficients(basis_study) #created from sim_data
prop <- find_prop_missing(missing_study, "confounder_1_missing")
CLQI_MI_results <- CLQI_MI(missing_dataset = missing_study,
basis_coefficients = basis_regression_coefficents,
specified_max = prop)
CLQI_MI_results
mean(CLQI_MI_results$coefficients) #rubin's rules
exp(mean(CLQI_MI_results$coefficients)) #rubin's rules and odds ratios
#necessary libraries
library(tidyverse)
library(ggplot2)
library(quantreg)
library(MASS)
library(purrr)
library(tictoc)
#logit and expit functions for myself
logit <- function(prob) {
value <- log(prob / (1 - prob))
return(value)
}
expit <- function(prob) {
value <- 1 / (1 + exp(-(prob)))
return(value)
}
#now some other transformation functions
log_quant_transform <- function(value, min, max) {
new_value <- log((value - min) / (max - value))
if (is.nan(new_value)) {return(NA)} #negative log values return an NA. Will be important to address
else {return(new_value)}
}
inv_log_quant_transform <- function(value, min, max) {
new_value <- (exp(value)*max + min) / (1+exp(value))
return(new_value)
}
coefficient_generator <- function(tau, num_studies,
a_0 = logit(0.3), a_1 = log(1.1),
b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2)) {
#create which number study we have
study_counts <- data.frame(id = 1:num_studies)
study_counts <- study_counts |>
mutate(study_num = paste("Study", id)) |>
dplyr::select(study_num)
#allow a_1 to vary
alpha_coefficients <- rnorm(num_studies, a_1, tau)
alpha_coefficients <- as.data.frame(alpha_coefficients) #make into data frame
#renaming for better binding experience + adding a_0... not important here
alpha_coefficients <- alpha_coefficients |>
mutate(a_0 = a_0) |>
rename(a_1 = "alpha_coefficients") |>
dplyr::select(a_0, a_1) #reordering
#now doing the beta coefficients
beta_vector <- c(b_1, b_2, b_3)
#variance-covariance matrix
matrix_size <- length(beta_vector)
diag_mat <- matrix(0, matrix_size, matrix_size)
diag(diag_mat) <- 1 #make all the diagonals 1 for the identity matrix
#lastly, we need to perform the calculation specified in section 3.1.4
beta_matrix <- tau * diag_mat
beta_coefficients <- mvrnorm(num_studies, beta_vector, beta_matrix)
beta_coefficients <- as.data.frame(beta_coefficients)
#renaming for better binding experience
beta_coefficients <- beta_coefficients |>
mutate(b_0 = b_0) |>
rename(b_1 = "V1", b_2 = "V2", b_3 = "V3") |>
dplyr::select(b_0, b_1, b_2, b_3)
#lastly, we need a range of degrees of freedom for the confounder generation
#there will be two just in case I want to use an F distribution
deg_freedom <- data.frame(df_1 = round(runif(num_studies, min = 3, max = 7)),
df_2 = round(runif(num_studies, min = 3, max = 7)))
#now combine these results
coefficients <- cbind(study_counts, alpha_coefficients, beta_coefficients, deg_freedom)
return(coefficients)
}
#this will be nested to make the list function cleaner to read
create_data <- function(sample_size, a_0, a_1, b_0, b_1, b_2, b_3, df_1, df_2) {
#initialize everything
n <- sample_size
#step 1: Base Binary Predictor
V <- rbinom(n, size = 1, prob = 0.4)
#step 2: cnfounder with a skewed distribution... Biomarker missing
C1 <- rchisq(n, df = df_1)
#C2 <- rnorm(n, mean = 75, sd = 7) #another confounder with a normal distribution. not used right now
#step 3: generating exposure variable based on confounders (probability)
E <- expit(a_0 + a_1*C1)
E_bin <- rbinom(n, size = 1, prob = E)
#step 4: generating outcome based on confounders, exposure, and base binary predictor
O <- expit(b_0 + b_1*E + b_2*V + b_3*C1)
O_bin <- rbinom(n, size = 1, prob = O)
#step 5: create dataset
my_data <- data.frame(
predictor = V,
confounder_1 = C1,
exposure = E_bin,
outcome = O_bin
)
return(my_data)
}
#recall all coefficients are stored in coefficient_generator funct
create_multiple_datasets <- function(study_coefficient_dataset, sample_size) {
my_list <- list() #initialize empty list
#pulling all values we need
a_0_values <- study_coefficient_dataset |> dplyr::pull(a_0)
a_1_values <- study_coefficient_dataset |> dplyr::pull(a_1)
b_0_values <- study_coefficient_dataset |> dplyr::pull(b_0)
b_1_values <- study_coefficient_dataset |> dplyr::pull(b_1)
b_2_values <- study_coefficient_dataset |> dplyr::pull(b_2)
b_3_values <- study_coefficient_dataset |> dplyr::pull(b_3)
#also degrees of freedom
df_1_values <- study_coefficient_dataset |> dplyr::pull(df_1)
df_2_values <- study_coefficient_dataset |> dplyr::pull(df_2)
for(i in 1:nrow(study_coefficient_dataset)) {
#do the data generating mechanism
a_0 <- a_0_values[i]
a_1 <- a_1_values[i]
b_0 <- b_0_values[i]
b_1 <- b_1_values[i]
b_2 <- b_2_values[i]
b_3 <- b_3_values[i]
#also the degrees of freedom
df_1 <- df_1_values[i]
df_2 <- df_2_values[i]
#apply the data generating mechanism function from the values above
my_data <- create_data(sample_size = sample_size,
a_0 = a_0,
a_1 = a_1,
b_0 = b_0,
b_1 = b_1,
b_2 = b_2,
b_3 = b_3,
df_1 = df_1,
df_2 = df_2)
#finally, add this dataset to our list
my_list[[i]] <- my_data
}
#return the list
return(my_list)
}
data_generating_mechanism <- function(sample_size, LD, tau) {
#first, create my studies based on coefficients
my_coefficients <- coefficient_generator(tau = tau, num_studies = 2) #2 is consistent here
my_studies <- create_multiple_datasets(my_coefficients, sample_size = sample_size)
#log transform our data
min = 0
max = ceiling(quantile(my_studies[[1]]$confounder_1, prob = 0.99))
suppressWarnings({ #a lot of warnings... some will be NA i know.
my_studies[[1]] <- my_studies[[1]] |>
mutate(confounder_1_transformed = sapply(confounder_1, log_quant_transform, min, max))
})
#define the LD and make data missing based on that
biomarker_values <- my_studies[[2]]$confounder_1
LoD_value <- as.numeric(quantile(biomarker_values, LD)) #must be given as a decimal
#now we made the data missing based on our given LD, and also transform this value
my_studies[[2]] <- my_studies[[2]] |>
mutate(confounder_1_missing = ifelse(confounder_1 >= LoD_value, confounder_1, NA))
#just to be able to look into my_coefficients later
my_studies <- list(my_studies[[1]], my_studies[[2]], my_coefficients)
return(my_studies) #return list of studies with missingness added
}
logistic_quantile_regression_coefficients <- function(basis_study) {
coefficient_data <- data.frame() #initiliaze empty data frame
for(i in seq(from = 0.01, to = 0.99, by = 0.01)) {
reg_coeff <- rq(confounder_1_transformed ~ exposure + predictor + outcome, data = basis_study, tau=i)
new_data <- data.frame(
b0 = reg_coeff$coefficients[1],
b_exposure = reg_coeff$coefficients[2],
b_predictor = reg_coeff$coefficients[3],
b_outcome = reg_coeff$coefficients[4],
quant = i)
coefficient_data <- rbind(coefficient_data, new_data) #add to new iterations
}
return(coefficient_data)
}
#make this into a function for easy use?
find_prop_missing <- function(missing_data_study, LD) {
prop_missing <- sum(is.na(missing_data_study[[LD]])) / nrow(missing_data_study)
prop_missing <- ceiling(prop_missing * 100)
return(prop_missing)
}
uniform_values <- function(specified_max) {
u <- runif(1, min = 1, max = specified_max) #aka from 0.01 to 0.99
#all the values we need from the uniform
floor_u <- floor(u)
mod_u <- (u - floor(u))
next_u <- ceiling(u)
#now putting this into a vector
my_u <- c(u, floor_u, mod_u, next_u)
return(my_u)
}
imputation_algorithm <- function(basis_coefficients, study_for_imputation,
var_for_imputation, row_index, u_vector) {
u_vector <- u_vector
floor_quantile <- basis_coefficients[u_vector[2], ] #floor
ceiling_quantile <- basis_coefficients[u_vector[4], ] #ceiling
#need to calculate regression values... really messy don't look at this
lower_quantile_value <- floor_quantile$b0 + (floor_quantile$b_predictor * study_for_imputation[row_index,]$predictor) +
(floor_quantile$b_exposure * study_for_imputation[row_index,]$exposure) + (floor_quantile$b_outcome * study_for_imputation[row_index,]$outcome)
upper_quantile_value <- ceiling_quantile$b0 + (ceiling_quantile$b_predictor * study_for_imputation[row_index,]$predictor) +
(ceiling_quantile$b_exposure * study_for_imputation[row_index,]$exposure) + (ceiling_quantile$b_outcome * study_for_imputation[row_index,]$outcome)
modulus <- u_vector[3]
imputation_value_transformed <- ((1-modulus)*lower_quantile_value) + (modulus*upper_quantile_value)
#lastly, untransform this value using the right min and max... organization is a headache
min_imp <- 0
missing_data_proportion <- find_prop_missing(missing_data_study = study_for_imputation,
LD = as.character(var_for_imputation)) / 100
#THIS IS THE ISSUE...
max_imp <- as.numeric(quantile(study_for_imputation$confounder_1, missing_data_proportion))
#then normal distribution to add random error... AHHHHHHH
#remember min is 0 and prop_missing gives us the right quantile for calculating max from study_for_imputation
imputed_value_regular <- inv_log_quant_transform(value = imputation_value_transformed,
min = min_imp,
max = max_imp)
return(imputed_value_regular)
}
fixed_MI <- function(num_iterations = 10, missing_dataset,
basis_coefficients, var_for_imputation, #var_for_imputation is no longer used
specified_max) {
b1_coefficients <- numeric(num_iterations)
b1_SE <- numeric(num_iterations)
for(i in 1:num_iterations) {
#generate ONE uniform value
my_uniform_values <- uniform_values(specified_max = specified_max)
fixed_MI_dataset <- missing_dataset |>
mutate(confounder_1_fixed_MI = ifelse(is.na(confounder_1_missing),
imputation_algorithm(basis_coefficients = basis_coefficients,
study_for_imputation = missing_dataset,
var_for_imputation = "confounder_1_missing",
row_index = seq_len(nrow(missing_dataset)),
u_vector = my_uniform_values),
confounder_1_missing)
)
#then run logistic regression
log_reg_result <- glm(outcome ~ exposure + predictor + confounder_1_fixed_MI,
data = fixed_MI_dataset,
family = "binomial")
#store the b1 parameter in a vector
b1_coefficients[i] <- log_reg_result$coefficients[2]
b1_SE[i] <- summary(log_reg_result)$coefficients[2,2]
}
#now put the parameter estimates and their associated variances in a dataframe and return that for each MI iteration
coefficients_and_SE <- data.frame(coefficients = b1_coefficients,
SE = b1_SE)
return(coefficients_and_SE)
}
#this is the CLQI algorithm, with some modifications to the u_vector idea
#for fixed MI, just move where the my_uniform_values gets called, so we use the same uniform value each time
#sadly this is not efficient, but for readability, and also for me to not go insane, this is what I did
CLQI_algorithm <- function(missing_dataset, basis_coefficients, specified_max) {
missing_dataset$confounder_1_CLQI <- NA #initialize a new variable here... base R ugh
for(row_index in 1:nrow(missing_dataset)) {
if(is.na(missing_dataset$confounder_1_missing[row_index])) {
my_uniform_values <- uniform_values(specified_max) #generate unique uniform values
imputed_value <- imputation_algorithm(basis_coefficients = basis_coefficients,
study_for_imputation = missing_dataset,
var_for_imputation = "confounder_1_missing",
row_index = row_index, #to get the right one
u_vector = my_uniform_values)
missing_dataset$confounder_1_CLQI[row_index] <- imputed_value
}
else {
missing_dataset$confounder_1_CLQI[row_index] <- missing_dataset$confounder_1_missing[row_index]
}
}
return(missing_dataset)
}
CLQI_MI <- function(num_iterations = 10, missing_dataset,
basis_coefficients, var_for_imputation, #var_for_imputation is no longer used
specified_max) {
b1_coefficients <- numeric(num_iterations) #initialize vector
b1_SE <- numeric(num_iterations) #initialize vector
for(i in 1:num_iterations) {
#run the algorithm
missing_dataset_imputed <- CLQI_algorithm(missing_dataset, basis_coefficients, specified_max)
#run logistic regression
log_reg_result <- glm(outcome ~ exposure + predictor + confounder_1_CLQI,
data = missing_dataset_imputed,
family = "binomial")
#extract exposure coefficients (b1)
b1_coefficients[i] <- log_reg_result$coefficients[2]
b1_SE[i] <- summary(log_reg_result)$coefficients[2,2]
}
#now make a dataframe for the coefficients and variance for each MI iteration
coefficients_and_SE <- data.frame(coefficients = b1_coefficients,
SE = b1_SE)
return(coefficients_and_SE)
}
constant_imputation <- function(missing_dataset, var_for_imputation, LD) {
missing_dataset_constanted <- missing_dataset |>
dplyr::mutate(confounder_1_sqrt2_imp = ifelse(is.na(confounder_1_missing),
(LD / sqrt(2)),
confounder_1_missing))
log_reg_result <- glm(outcome ~ exposure + predictor + confounder_1_sqrt2_imp,
data = missing_dataset_constanted,
family = "binomial")
constant_imp_result <- c(log_reg_result$coefficients[2], summary(log_reg_result)$coefficients[2,2])
return(constant_imp_result) #for the exposure variable
}
complete_case_analysis <- function(missing_dataset, var_for_imputation) {
CC_missing_dataset <- missing_dataset |>
filter(!is.na(confounder_1_missing)) #only keep the values where confounder_1_missing is still observed
log_reg_result <- glm(outcome ~ exposure + predictor + confounder_1_missing,
data = CC_missing_dataset,
family = "binomial")
CC_results <- c(log_reg_result$coefficients[2], summary(log_reg_result)$coefficients[2,2])
return(CC_results) #for the exposure variable
}
rubin_rules <- function(estimate_and_variance) {
#the parameter result
coefficient_MI <- mean(estimate_and_variance$coefficients)
#the variance result
variance_MI <- mean((estimate_and_variance$SE)^2)
SE_MI <- sqrt(variance_MI)
#return a vector, where [1] is the estimate and [2] is the variance
my_vector <- c(coefficient_MI, SE_MI)
return(my_vector)
}
set.seed(605) #set seed for reproducibility
my_sample_size = 500
my_tau = 0.01
one_iteration <- function(specified_sample_size, specified_tau, specified_LD) {
#generate the data
#create the data here so we can do the same imputation with the same data
my_data <- data_generating_mechanism(sample_size = specified_sample_size,
LD = specified_LD,
tau = specified_tau)
#our studies!
basis_study <- as.data.frame(my_data[[1]])
missing_study <- as.data.frame(my_data[[2]])
the_LD <- min(missing_study$confounder_1_missing, na.rm=TRUE)
#CLQI
#the basis dataset runs conditional logistic quantile regression
basis_regression_coefficents <- logistic_quantile_regression_coefficients(basis_study) #created from my_data
prop <- find_prop_missing(missing_study, "confounder_1_missing")
#the CLQI_MI results
CLQI_MI_results <- CLQI_MI(num_iterations = 10,
missing_dataset = missing_study, #created from my_data
basis_coefficients = basis_regression_coefficents,
var_for_imputation = "confounder_1_missing", #again not used
specified_max = prop)
#combine these results using rubin's rules (since the function returns the 10 iterations from MI)
CLQI_MI_results <- rubin_rules(CLQI_MI_results)
#the fixed MI results
fixed_MI_results <- fixed_MI(num_iterations = 10,
missing_dataset = missing_study,
basis_coefficients = basis_regression_coefficents,
specified_max = prop)
#combine these results using rubin's rules (since the function returns the 10 iterations from MI)
fixed_MI_results <- rubin_rules(fixed_MI_results)
###############################################################################################
#the constant imputation results
constant_imputation_sqrt_2_results <- constant_imputation(missing_dataset = missing_study,
LD = the_LD)
#the complete case results
complete_case_results <- complete_case_analysis(missing_dataset = missing_study)
###############################################################################################
#turning this into a vector: the specific order of our methods will matter
imputation_results_vector <- data.frame(estimate_CLQI = CLQI_MI_results[1],
SE_CLQI = CLQI_MI_results[2],
estimate_fixed_MI = fixed_MI_results[1],
SE_fixed_MI = fixed_MI_results[2],
estimate_constant = constant_imputation_sqrt_2_results[1],
SE_constant = constant_imputation_sqrt_2_results[2],
estimate_CC = complete_case_results[1],
SE_CC = complete_case_results[2])
return(imputation_results_vector)
}
repetera_full_simulation <- function(specified_sample_size, specified_tau, specified_LD, sim_size, b1_true) {
sim_results <- purrr::map_dfr(1:sim_size, ~ {
# We go through one iteration of the simulation
parameter_est <- one_iteration(specified_sample_size = specified_sample_size,
specified_tau = specified_tau,
specified_LD = specified_LD)
# Return the result of the current iteration
parameter_est
})
#now for coverage: need to calculate based on a condition
t_star <- qt(0.975, df = specified_sample_size - 1)
#after, we will calculate the statistics we desire... STILL HAVE NOT DONE COVERAGE YET
sim_results_send <- sim_results |>
mutate(bias_CLQI = estimate_CLQI - b1_true,
MSE_CLQI = (bias_CLQI)^2 + (SE_CLQI)^2,
coverage_CLQI = ifelse(estimate_CLQI - (t_star*SE_CLQI) <= b1_true & b1_true <= estimate_CLQI + (t_star*SE_CLQI), 1, 0),
bias_fixed_MI = estimate_fixed_MI - b1_true,
MSE_fixed_MI = (bias_fixed_MI)^2 + (SE_fixed_MI)^2,
coverage_fixed_MI = ifelse(estimate_fixed_MI - (t_star*SE_fixed_MI) <= b1_true & b1_true <= estimate_fixed_MI + (t_star*SE_fixed_MI), 1, 0),
bias_constant = estimate_constant - b1_true,
MSE_constant = (bias_constant)^2 + (SE_constant)^2,
coverage_constant = ifelse(estimate_constant - (t_star*SE_constant) <= b1_true & b1_true <= estimate_constant + (t_star*SE_constant), 1, 0),
bias_CC = estimate_CC - b1_true,
MSE_CC = (bias_CC)^2 + (SE_CC)^2,
coverage_CC = ifelse(estimate_CC - (t_star*SE_CC) <= b1_true & b1_true <= estimate_CC + (t_star*SE_CC), 1, 0)
)
return(sim_results_send)
}
tictoc::tic() #start the timer to check
sim_setting_1_results <- repetera_full_simulation(specified_sample_size = 500,
specified_tau = 0,
specified_LD = 0.50, #30% missing data
sim_size = 10,
b1_true = log(1.1))
tictoc::toc() # Finish the computation time
sim_setting_1_results
tictoc::tic() #start the timer to check
sim_setting_1_results <- repetera_full_simulation(specified_sample_size = 500,
specified_tau = 0,
specified_LD = 0.50, #30% missing data
sim_size = 1000,
b1_true = log(1.1))
tictoc::toc() # Finish the computation time
#save the results and look at them!
saveRDS(sim_setting_1_results, file="./simulation_results/sim_setting_1.rds")
sim_setting_1_results <- readRDS(file="./simulation_results/sim_setting_1.rds")
ggplot(data=sim_setting_1_results, aes(x=bias_fixed_MI)) +
geom_density() +
geom_vline(aes(xintercept = mean(bias_CLQI)), color = "red", linetype = "dashed", size = 1) +
theme_bw()
sim_setting_1_results
ggplot(data=sim_setting_1_results, aes(x=bias_CLQI)) +
geom_density() +
geom_vline(aes(xintercept = mean(bias_CLQI)), color = "red", linetype = "dashed", size = 1) +
theme_bw()
mean(sim_setting_1_results$bias_CLQI)
sim_setting_1_results_bias <- sim_setting_1_results |>
pivot_longer(cols = c(bias_fixed_MI, bias_CLQI, bias_constant, bias_CC),
names_to = "bias_type",
values_to = "bias_value")
# Plot the density of each bias type with separate graphs
ggplot(sim_setting_1_results_bias, aes(x = bias_value, fill = bias_type)) +
geom_density(alpha = 0.25) +  # Adjust transparency with alpha
geom_vline(data = sim_setting_1_results_bias %>%
group_by(bias_type) %>%
summarize(mean_bias = mean(bias_value)),
aes(xintercept = mean_bias, color = bias_type),
linetype = "dashed", size = 1) +
theme_bw() +
labs(title = "Density Plots of Bias Types", x = "Bias Value", y = "Density") +
scale_fill_manual(values = c("blue", "red", "green", "purple")) +  # Adjust colors as desired
scale_color_manual(values = c("blue", "red", "green", "purple")) +  # Same color scheme for the lines
facet_wrap(~bias_type, scales = "free")  # Separate graphs for each bias type
sim_setting_1_results_MSE <- sim_setting_1_results |>
pivot_longer(cols = c(MSE_fixed_MI, MSE_CLQI, MSE_constant, MSE_CC),
names_to = "MSE_type",
values_to = "MSE_value")
# Plot the density of each bias type
ggplot(sim_setting_1_results_MSE, aes(x = MSE_value, fill = MSE_type)) +
geom_density(alpha = 0.25) +  # Adjust transparency with alpha
geom_vline(data = sim_setting_1_results_MSE %>%
group_by(MSE_type) %>%
summarize(mean_MSE = mean(MSE_value)),
aes(xintercept = mean_MSE, color = MSE_type),
linetype = "dashed", size = 1) +
theme_bw() +
labs(title = "Density Plots of MSE Types", x = "MSE Value", y = "Density") +
scale_fill_manual(values = c("blue", "red", "green", "purple")) +  # Adjust colors as desired
scale_color_manual(values = c("blue", "red", "green", "purple")) + # Same color scheme for the lines
facet_wrap(~MSE_type, scales = "free")  +
xlim(0,0.5)
sim_setting_1_coverage <- sim_setting_1_results |>
summarize(cov_perc_CLQI = sum(coverage_CLQI) / nrow(sim_setting_1_results),
cov_perc_fixed_MI = sum(coverage_fixed_MI) / nrow(sim_setting_1_results),
cov_perc_constant = sum(coverage_constant) / nrow(sim_setting_1_results),
cov_perc_CC = sum(coverage_CC) / nrow(sim_setting_1_results))
sim_setting_1_coverage
