```{r}
set.seed(605)
library(tidyverse)
library(MASS) #multivariate normal beta coefficient generation
```

```{r}
#logit and expit functions for myself
logit <- function(prob) {
  value <- log(prob / (1 - prob))
  return(value)
}

expit <- function(prob) {
  value <- 1 / (1 + exp(-(prob)))
  return(value)
}
```

```{r, eval=FALSE}
#function to create data: one study case for demonstration purposes only
create_data <- function(sample_size, 
                        a_0 = logit(0.3), a_1 = log(1.1), 
                        b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2)) {
  
  #initialize everything
  n <- sample_size
  
  #step 1: Base Binary Predictor
  V <- rbinom(n, size = 1, prob = 0.4)
  
  #step 2: cnfounder with a skewed distribution... Biomarker missing
  C1 <- rchisq(n, df = 5) 
  C2 <- rnorm(n, mean = 75, sd = 7) #another confounder with a normal distribution. not used right now
  
  #step 3: generating exposure variable based on confounders (probability)
  E <- expit(a_0 + a_1*C1)
  
  #step 4: generating outcome based on confounders, exposure, and base binary predictor
  O <- expit(b_0 + b_1*E + b_2*V + b_3*C1)
  
  #step 5: create dataset
  
  my_data <- data.frame(
    predictor = V,
    confounder_1 = C1,
    confounder_2 = C2,
    exposure = E,
    outcome = O
  )
  
  return(my_data)
} 
```

```{r, eval=FALSE}
#validation
toy_data <- create_data(1000)

#doing the regression
toy_results <- glm(outcome ~ exposure + predictor + confounder_1, 
                   family="binomial", 
                   data=toy_data)

summary(toy_results) #the coeffiecents are exactly the logit inputs we gave in the function! 
```

```{r, eval=FALSE}
#our variability
tau_2 <- 0.01 

#beta coefficients
b_1 <- log(1.5)
b_2 <- log(0.7)
b_3 <- log(1.2)
beta_vector <- c(b_1, b_2, b_3)

#variance-covariance matrix
matrix_size <- length(beta_vector)
diag_mat <- matrix(0, matrix_size, matrix_size) 
diag(diag_mat) <- 1 #make all the diagonals 1 for the identity matrix
  
#lastly, we need to perform the calculation specified in section 3.1.4
beta_matrix <- sqrt(tau_2) * diag_mat

#multivariate normal function
a <- mvrnorm(1000, beta_vector, beta_matrix)
a <- as.data.frame(a)

#checking that the averages of the coefficients are close to our original values
exp(mean(a$V1))
exp(mean(a$V2))
exp(mean(a$V3))
#it works!!
```


Now we will combine these two ideas to create multiple data sets with multiple confounding mechanisms

```{r}
coefficient_generator <- function(tau, num_studies, 
                                  a_0 = logit(0.3), a_1 = log(1.1), 
                                  b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2)) {
    
  #create which number study we have
  study_counts <- data.frame(id = 1:num_studies)
  
  study_counts <- study_counts |>
    mutate(study_num = paste("Study", id)) |>
    dplyr::select(study_num)
  
  #allow a_1 to vary
  alpha_coefficients <- rnorm(num_studies, a_1, tau)
  alpha_coefficients <- as.data.frame(alpha_coefficients) #make into data frame
  
      #renaming for better binding experience + adding a_0... not important here
      alpha_coefficients <- alpha_coefficients |> 
        mutate(a_0 = a_0) |>
        rename(a_1 = "alpha_coefficients") |>
        dplyr::select(a_0, a_1) #reordering
  
      
  #now doing the beta coefficients
  beta_vector <- c(b_1, b_2, b_3)
  
    #variance-covariance matrix
    matrix_size <- length(beta_vector)
    diag_mat <- matrix(0, matrix_size, matrix_size) 
    diag(diag_mat) <- 1 #make all the diagonals 1 for the identity matrix
    
    #lastly, we need to perform the calculation specified in section 3.1.4
    beta_matrix <- tau * diag_mat
    beta_coefficients <- mvrnorm(num_studies, beta_vector, beta_matrix)
    beta_coefficients <- as.data.frame(beta_coefficients)
    
      #renaming for better binding experience
      beta_coefficients <- beta_coefficients |> 
        mutate(b_0 = b_0) |>
        rename(b_1 = "V1", b_2 = "V2", b_3 = "V3") |>
        dplyr::select(b_0, b_1, b_2, b_3)
      
  #we need to set a LoD value for these studies that has slight variation
  LoD <- data.frame(LoD = round(runif(num_studies, min = 0.8, max = 1.2), 3))
  
  #lastly, we need a range of degrees of freedom for the confounder generation
  #there will be two just in case I want to use an F distribution
  deg_freedom <- data.frame(df_1 = round(runif(num_studies, min = 3, max = 7)),
                            df_2 = round(runif(num_studies, min = 3, max = 7)))
  
  #now combine these results
  coefficients <- cbind(study_counts, alpha_coefficients, beta_coefficients, LoD, deg_freedom)
  
  return(coefficients)
}

#coefficient_generator(tau = 0.01, num_studies = 30)
```

```{r, eval=FALSE}
#how to work with this in the next function: we need to take in a dataframe created by coefficient_generator... two steps!

iteration <- 1

study_1 <- as.numeric(toy_coefficients[iteration,])
study_1
```

Not 100% sure how I'm going to structure this function...
I also don't know if I'm going to keep the sample size homogeneous... maybe allow a vector to be passed through, or create random var.

The function's steps:
1) Take in a dataset that has study_num and the alpha/beta coefficients 
  *note that a_0 and sample_size need to be added in this function... maybe add a_0 in other function?*
2) Iterate through the amount of studies we have and create data (with the mechanism described way above)
3) Store one iteration in a list of the same index and keep going
4) The output should be a list of however mnany study_num we had in the previous function

*note that this function does not do it all. you need to create the study_coefficient_dataset prior to this

```{r, eval=FALSE}
#my testing realm
my_list[[1]] <- data.frame(x = "ooga", y = "booga")
my_list[[1]]

#test case: take study 1 from our generated coefficients and apply this function onto it!
test_case <- coefficient_generator(tau = 0.01, num_studies = 30) |>
  dplyr::slice_head(n = 2) 

test_case[1,]$a_0
test_case[1,]$a_1
test_case[1,]$b_0
test_case[1,]$b_1
test_case[1,]$a_0
test_case[1,]$a_0


#another way to do this with dplyr... might be better, since we create vectors
a_0_values <- test_case |> dplyr::pull(a_0)
a_0_values[1]
```

```{r}
#this will be nested to make the list function cleaner to read
create_data <- function(sample_size, a_0, a_1, b_0, b_1, b_2, b_3, df_1, df_2) {
  
  #initialize everything
  n <- sample_size
  
  #step 1: Base Binary Predictor
  V <- rbinom(n, size = 1, prob = 0.4)
  
  #step 2: cnfounder with a skewed distribution... Biomarker missing
  C1 <- rchisq(n, df = df_1) 
  C2 <- rnorm(n, mean = 75, sd = 7) #another confounder with a normal distribution. not used right now
  
  #step 3: generating exposure variable based on confounders (probability)
  E <- expit(a_0 + a_1*C1)
  
  #step 4: generating outcome based on confounders, exposure, and base binary predictor
  O <- expit(b_0 + b_1*E + b_2*V + b_3*C1)
  
  #step 5: create dataset
  
  my_data <- data.frame(
    predictor = V,
    confounder_1 = C1,
    confounder_2 = C2,
    exposure = E,
    outcome = O
  )
  
  return(my_data)
} 
```

```{r}
#recall all coefficients are stored in coefficient_generator funct
create_multiple_datasets <- function(study_coefficient_dataset, sample_size) {
  my_list <- list() #initialize empty list
  
  #pulling all values we need
  a_0_values <- study_coefficient_dataset |> dplyr::pull(a_0)
  a_1_values <- study_coefficient_dataset |> dplyr::pull(a_1)
  b_0_values <- study_coefficient_dataset |> dplyr::pull(b_0)
  b_1_values <- study_coefficient_dataset |> dplyr::pull(b_1)
  b_2_values <- study_coefficient_dataset |> dplyr::pull(b_2)
  b_3_values <- study_coefficient_dataset |> dplyr::pull(b_3)
  
  #also degrees of freedom
  df_1_values <- study_coefficient_dataset |> dplyr::pull(df_1)
  df_2_values <- study_coefficient_dataset |> dplyr::pull(df_2) #only if necessary
  
  for(i in 1:nrow(study_coefficient_dataset)) {
    #do the data generating mechanism
    a_0 <- a_0_values[i]
    a_1 <- a_1_values[i]
    b_0 <- b_0_values[i]
    b_1 <- b_1_values[i]
    b_2 <- b_2_values[i]
    b_3 <- b_3_values[i]
    
    #also the degrees of freedom
    df_1 <- df_1_values[i]
    df_2 <- df_2_values[i]
    
    #apply the data generating mechanism function from the values above
    my_data <- create_data(sample_size = sample_size, 
                           a_0 = a_0, a_1 = a_1, 
                           b_0 = b_0, b_1 = b_1, b_2 = b_2, b_3 = b_3,
                           df_1 = df_1, df_2 = df_2)
    
    
    #finally, add this dataset to our list
    my_list[[i]] <- my_data
  
  }
  
  #return the list
  return(my_list)
}
```

```{r}
#the two functions in play... creates all our meta analysis data, as well as a separate dataset
#showing the underlying coefficients that make this all work!

my_coefficients <- coefficient_generator(tau = 0.01, num_studies = 5)
meta_analysis_data <- create_multiple_datasets(my_coefficients, sample_size = 500)
```

Now we need a mechanism to make these values go systematically missing for a certain range
Say the LoD ranges from 0.8 to 1.2 across different study sites
We have at least one study (say study 1) that has all observed values
*we can also test if this method works if there is 10% missing observations in this range*
We need to know from the other study cites how many quantiles they need for imputation

```{r, eval=FALSE}
#working with lists is the same as you would expect (maybe)
length(meta_analysis_data)
meta_analysis_data[1]

#and also an idea for the LoD to range from 0.8 to 1.2
runif(1, min = 0.8, max = 1.2)

#function idea
LoD_value <- runif(1, min = 0.8, max = 1.2)

test <- meta_analysis_data[[1]] |>
  mutate(confounder_1_missing = ifelse(confounder_1 >= LoD_value, confounder_1, NA))
  
sum(is.na(test$confounder_1_missing))
```

```{r}
#my_coefficients will have a LoD specified from its generation
LoD_maker <- function(my_list, my_coefficients) {
  
  #maybe change this to purrr, but for readability for non-R users, this seems like a better option
  for(i in 1:length(my_list)) {
  
    my_list[[i]] <- my_list[[i]] |>
      mutate(confounder_1_missing = ifelse(confounder_1 >= my_coefficients$LoD[i], confounder_1, NA))
    
  }
  return(my_list)
}
```

```{r}
meta_analysis_data <- LoD_maker(meta_analysis_data, my_coefficients)
```

This is a function to count how much data is missing in each study
This value will be saved in the coefficients dataset

```{r}
missing_data_counter <- function(my_list, my_coefficients) {
  my_coefficients$num_missing <- NA #initialize new variable
  
  #maybe change this to purrr, but for readability for non-R users, this seems like a better option
  for(i in 1:length(my_list)) {
    missing_count <- sum(is.na(my_list[[i]]$confounder_1_missing))
    my_coefficients$num_missing[i] <- missing_count #save this value in my_coefficients
  }
  return(my_coefficients)
  
}

my_coefficients <- missing_data_counter(meta_analysis_data, my_coefficients)
```

