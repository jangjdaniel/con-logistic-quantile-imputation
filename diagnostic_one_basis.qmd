```{r}
#necessary libraries
library(tidyverse)
library(quantreg)
library(MASS)
library(purrr)
```

For simplicity, we are in the case where we don't need multiple datasets, just two
*these functions are from data_generating_mechanism.qmd*

```{r}
#logit and expit functions for myself
logit <- function(prob) {
  value <- log(prob / (1 - prob))
  return(value)
}

expit <- function(prob) {
  value <- 1 / (1 + exp(-(prob)))
  return(value)
}

#now some other transformation functions
log_quant_transform <- function(value, min, max) {
  new_value <- log((value - min) / (max - value))
  
  if (is.nan(new_value)) {return(NA)} 
  else {return(new_value)}
}

inv_log_quant_transform <- function(value, min, max) {
  new_value <- (exp(value)*max + min) / (1+exp(value))
  return(new_value)
}
```

```{r}
coefficient_generator <- function(tau, num_studies, 
                                  a_0 = logit(0.3), a_1 = log(1.1), 
                                  b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2)) {
    
  #create which number study we have
  study_counts <- data.frame(id = 1:num_studies)
  
  study_counts <- study_counts |>
    mutate(study_num = paste("Study", id)) |>
    dplyr::select(study_num)
  
  #allow a_1 to vary
  alpha_coefficients <- rnorm(num_studies, a_1, tau)
  alpha_coefficients <- as.data.frame(alpha_coefficients) #make into data frame
  
      #renaming for better binding experience + adding a_0... not important here
      alpha_coefficients <- alpha_coefficients |> 
        mutate(a_0 = a_0) |>
        rename(a_1 = "alpha_coefficients") |>
        dplyr::select(a_0, a_1) #reordering
  
      
  #now doing the beta coefficients
  beta_vector <- c(b_1, b_2, b_3)
  
    #variance-covariance matrix
    matrix_size <- length(beta_vector)
    diag_mat <- matrix(0, matrix_size, matrix_size) 
    diag(diag_mat) <- 1 #make all the diagonals 1 for the identity matrix
    
    #lastly, we need to perform the calculation specified in section 3.1.4
    beta_matrix <- tau * diag_mat
    beta_coefficients <- mvrnorm(num_studies, beta_vector, beta_matrix)
    beta_coefficients <- as.data.frame(beta_coefficients)
    
      #renaming for better binding experience
      beta_coefficients <- beta_coefficients |> 
        mutate(b_0 = b_0) |>
        rename(b_1 = "V1", b_2 = "V2", b_3 = "V3") |>
        dplyr::select(b_0, b_1, b_2, b_3)
      
  #we need to set a LoD value for these studies that has slight variation
  LoD <- data.frame(LoD = round(runif(num_studies, min = 0.8, max = 1.2), 3))
  
  #lastly, we need a range of degrees of freedom for the confounder generation
  #there will be two just in case I want to use an F distribution
  deg_freedom <- data.frame(df_1 = round(runif(num_studies, min = 3, max = 7)),
                            df_2 = round(runif(num_studies, min = 3, max = 7)))
  
  #now combine these results
  coefficients <- cbind(study_counts, alpha_coefficients, beta_coefficients, LoD, deg_freedom)
  
  return(coefficients)
}

#coefficient_generator(tau = 0.01, num_studies = 30)
```

```{r}
#this will be nested to make the list function cleaner to read
create_data <- function(sample_size, a_0, a_1, b_0, b_1, b_2, b_3, df_1, df_2) {
  
  #initialize everything
  n <- sample_size
  
  #step 1: Base Binary Predictor
  V <- rbinom(n, size = 1, prob = 0.4)
  
  #step 2: cnfounder with a skewed distribution... Biomarker missing
  C1 <- rchisq(n, df = df_1) 
  C2 <- rnorm(n, mean = 75, sd = 7) #another confounder with a normal distribution. not used right now
  
  #step 3: generating exposure variable based on confounders (probability)
  E <- expit(a_0 + a_1*C1)
  E_bin <- rbinom(n, size = 1, prob = E)
  
  #step 4: generating outcome based on confounders, exposure, and base binary predictor
  O <- expit(b_0 + b_1*E + b_2*V + b_3*C1)
  O_bin <- rbinom(n, size = 1, prob = O)
  
  #step 5: create dataset
  
  my_data <- data.frame(
    predictor = V,
    confounder_1 = C1,
    confounder_2 = C2,
    exposure = E_bin,
    outcome = O_bin
  )
  
  return(my_data)
} 
```

```{r}
#recall all coefficients are stored in coefficient_generator funct
create_multiple_datasets <- function(study_coefficient_dataset, sample_size) {
  my_list <- list() #initialize empty list
  
  #pulling all values we need
  a_0_values <- study_coefficient_dataset |> dplyr::pull(a_0)
  a_1_values <- study_coefficient_dataset |> dplyr::pull(a_1)
  b_0_values <- study_coefficient_dataset |> dplyr::pull(b_0)
  b_1_values <- study_coefficient_dataset |> dplyr::pull(b_1)
  b_2_values <- study_coefficient_dataset |> dplyr::pull(b_2)
  b_3_values <- study_coefficient_dataset |> dplyr::pull(b_3)
  
  #also degrees of freedom
  df_1_values <- study_coefficient_dataset |> dplyr::pull(df_1)
  df_2_values <- study_coefficient_dataset |> dplyr::pull(df_2) #only if necessary
  
  for(i in 1:nrow(study_coefficient_dataset)) {
    #do the data generating mechanism
    a_0 <- a_0_values[i]
    a_1 <- a_1_values[i]
    b_0 <- b_0_values[i]
    b_1 <- b_1_values[i]
    b_2 <- b_2_values[i]
    b_3 <- b_3_values[i]
    
    #also the degrees of freedom
    df_1 <- df_1_values[i]
    df_2 <- df_2_values[i]
    
    #apply the data generating mechanism function from the values above
    my_data <- create_data(sample_size = sample_size, 
                           a_0 = a_0, a_1 = a_1, 
                           b_0 = b_0, b_1 = b_1, b_2 = b_2, b_3 = b_3,
                           df_1 = df_1, df_2 = df_2)
    
    
    #finally, add this dataset to our list
    my_list[[i]] <- my_data
  
  }
  
  #return the list
  return(my_list)
}
```



Generate my data now

```{r}
set.seed(605)

#don't forget to load the three functions necessary to make this work
my_coefficients_basic <- coefficient_generator(tau = 0.01, num_studies = 2)
my_studies_basic <- create_multiple_datasets(my_coefficients_basic, sample_size = 500)
```

```{r}
#doesn't matter which study is the basis study
basis_study <- as.data.frame(my_studies_basic[1])
missing_study <- as.data.frame(my_studies_basic[2])

#get rid of the mess
rm(my_studies_basic)
```

Now simple mechanism to make LoD data for the missing study. Also count the prop of missing data

```{r}
#look at my_coefficients and see that randomly selected LoD is 0.912
missing_study <- missing_study |>
  mutate(confounder_1_missing = ifelse(confounder_1 >= 0.912, confounder_1, NA))

#count proportion of missingness here: 17.6%
prop_missing = sum(is.na(missing_study$confounder_1_missing)) / nrow(missing_study)
prop_missing
```

View what the confounder 1 looks like

```{r}
ggplot(missing_study) +
  geom_histogram(aes(x = confounder_1, fill = "original"), alpha = 0.5) +
  geom_histogram(aes(x = confounder_1_missing, fill = "imputed"), alpha = 0.5) + 
  xlim(0,10)
```


`STEP 1`: Perform a logistic transform on the Biomarker to impute.

```{r}
min = 0
max = ceiling(quantile(basis_study$confounder_1, prob = 0.99))

#the transformation of confounder 1
basis_study <- basis_study |>
  mutate(confounder_1_transformed = sapply(confounder_1, log_quant_transform, min, max))

#some NaN's are produced, but that is to be expected
```

`STEP 2`: Perform Conditional Logistic Quantile Regression

```{r}
#test with 1st quartile, median, and 3rd quartile: need to transform them back

#initiliaze empty data frame
coefficient_data <- data.frame()

for(i in seq(0.01:0.99, by=0.01)) {
  reg_coeff <- rq(confounder_1_transformed ~ exposure + predictor + outcome, data = basis_study, tau=i)
  
  new_data <- data.frame(
    b0 = reg_coeff$coefficients[1],
    b_exposure = reg_coeff$coefficients[2],
    b_predictor = reg_coeff$coefficients[3],
    b_outcome = reg_coeff$coefficients[4],
    quant = i
  )
  
  coefficient_data <- rbind(coefficient_data, new_data) #add to new iterations
}
```

```{r, eval=FALSE}
reg_test <- rq(confounder_1_transformed ~ predictor + exposure + outcome, data = basis_study, tau=0.44)
summary(reg_test) #is there a way to extract variance here?
```

`STEP 2.5`: Store these results in a data frame

```{r}
#we already did this, so we're just going to clean up
rm(new_data)
rm(reg_coeff)

coefficient_data
```


`STEP 3`: Send these coefficients to an agreed-upon central study site.
- Some extra steps here, but we are keeping it at |A| = 1 for now
`STEP 4`: Send the list of regression coefficients to Missing-Data Datasets. 
`STEP 4.5`: Each individual Missing-Data Study calculates FMj, and round this value to the nearest thousandths place. 

```{r}
#we had this code previously, but it's repeated here for clarification
prop_missing = sum(is.na(missing_study$confounder_1_missing)) / nrow(missing_study)
prop_missing = ceiling(prop_missing * 100)
prop_missing

#make this into a function for easy use?

find_prop_missing <- function(missing_data_study, LD) {
  prop_missing <- sum(is.na(missing_data_study[[LD]])) / nrow(missing_data_study)
  prop_missing <- ceiling(prop_missing * 100)
  
  return(prop_missing)
}

#should spit out 21... works!
find_prop_missing(missing_data_study = missing_study, LD = "confounder_1_missing")
```

`STEP 5`: Perform the Imputation Algorithm
In the future, this will be the basis of the package, so we will need better methods
Give a warning if prop_missing < 0.05, saying "MI is not recommended for less than 5% missing data"

```{r}
uniform_values <- function() {
  u <- runif(1, min = 1, max = 99) #aka from 0.01 to 0.99
  
  #all the values we need from the uniform
  floor_u <- floor(u)
  mod_u <- (u - floor(u))
  next_u <- ceiling(u)
  
  #now putting this into a vector
  my_u <- c(u, floor_u, mod_u, next_u)
  return(my_u)
}
```

The function that is implementing CLQI

```{r}
fixed_MI <- function(basis_coefficients, study_for_imputation, 
                     var_for_imputation, row_index) {
  
  #random draw from a uniform dist and extract important information
  u_vector <- uniform_values() 
  floor_quantile <- coefficient_data[u_vector[2], ] #floor
  ceiling_quantile <- coefficient_data[u_vector[4], ] #ceiling
  
  #need to calculate regression values... really messy don't look at this
  lower_quantile_value <- floor_quantile$b0 + (floor_quantile$b_predictor * missing_study[row_index,]$predictor) +
    (floor_quantile$b_exposure * missing_study[row_index,]$exposure) + (floor_quantile$b_outcome * missing_study[row_index,]$outcome)

  upper_quantile_value <- ceiling_quantile$b0 + (ceiling_quantile$b_predictor * missing_study[row_index,]$predictor) +
    (ceiling_quantile$b_exposure * missing_study[row_index,]$exposure) + (ceiling_quantile$b_outcome * missing_study[row_index,]$outcome)

  modulus <- u_vector[3]
  imputation_value_transformed <- ((1-modulus)*lower_quantile_value) + (modulus*upper_quantile_value)
  
  #lastly, untransform this value using the right min and max... organization is a headache
  min_imp <- 0
  missing_data_proportion <- find_prop_missing(missing_data_study = study_for_imputation, 
                                               LD = as.character(var_for_imputation)) / 100
  
  #THIS IS THE ISSUE...
  max_imp <- as.numeric(quantile(missing_study$confounder_1, missing_data_proportion))
  #then normal distribution to add random error... AHHHHHHH
  
  #remember min is 0 and prop_missing gives us the right quantile for calculating max from study_for_imputation
  imputed_value_regular <- inv_log_quant_transform(value = imputation_value_transformed, 
                                                   min = min_imp, 
                                                   max = max_imp)
  
  unif_val <- u_vector[1]
  
  return(unif_val)
}
```

Simple tidyverse thing to perform this imputation, just once...

```{r}
missing_study <- missing_study |>
  mutate(confounder_1_CLQI = ifelse(is.na(confounder_1_missing), 
                                    CLQI(basis_coefficients = coefficient_data,
                                         study_for_imputation = missing_study,
                                         var_for_imputation = "confounder_1_missing",
                                         row_index = seq_len(nrow(missing_study))), 
                                    confounder_1_missing))
```

```{r}
#checking stuff here
ggplot(data=missing_study) +
  geom_histogram(aes(x = confounder_1_CLQI, fill = "imputed"), alpha = 0.5) + 
  geom_histogram(aes(x = confounder_1, fill = "original"), alpha = 0.5) + 
  xlim(-1,3)
```

```{r}
#another diagnostic
missing_study <- missing_study |>
  mutate(only_imputed = ifelse(is.na(confounder_1_missing), confounder_1_CLQI, NA))

ggplot(missing_study) +
  geom_histogram(aes(x=only_imputed))
```

```{r}
summary(missing_study$only_imputed)
```

`STEP 7`: Perform logistic regression (or anything else you’re looking at) using this newly imputed variable and obtain the parameter of interest

```{r, eval=FALSE}
#"test case", non-function version
log_reg_result <- glm(outcome ~ exposure + confounder_1 + confounder_2, 
                      data = missing_study,
                      family="binomial")

log_odds_b1 <- log_reg_result$coefficients[2] #extracting the exposure variable coefficient
odds_b1 <- exp(log_odds_b1)

odds_b1 #NICE!!!!!!
```

Comparison with the imputed values

```{r, eval=FALSE}
#"test case", non-function version
log_reg_result <- glm(outcome ~ exposure + confounder_1_CLQI + confounder_2, 
                      data = missing_study,
                      family="binomial")

log_odds_b1 <- log_reg_result$coefficients[2] #extracting the exposure variable coefficient
odds_b1 <- exp(log_odds_b1)

odds_b1
```

Constant Imputation with sqrt(2)

```{r, eval=FALSE}
#"test case", non-function version

log_reg_result <- glm(outcome ~ exposure + confounder_1_constant_imputed + confounder_2, 
                      data = missing_study,
                      family="binomial")

log_odds_b1 <- log_reg_result$coefficients[2] #extracting the exposure variable coefficient
odds_b1 <- exp(log_odds_b1)

odds_b1
```

Now a complete case version

```{r, eval=FALSE}
#"test case", non-function version
missing_study_complete_case <- missing_study |>
  filter(!is.na(confounder_1_missing))

log_reg_result <- glm(outcome ~ exposure + confounder_1 + confounder_2, 
                      data = missing_study_complete_case,
                      family="binomial")

log_odds_b1 <- log_reg_result$coefficients[2] #extracting the exposure variable coefficient
odds_b1 <- exp(log_odds_b1)

odds_b1
```

More generally, don't use confounder 1: super terrible, even worse than complete case

```{r, eval=FALSE}
#"test case", non-function version
log_reg_result <- glm(outcome ~ exposure + confounder_2, 
                      data = missing_study_complete_case,
                      family="binomial")

log_odds_b1 <- log_reg_result$coefficients[2] #extracting the exposure variable coefficient
odds_b1 <- exp(log_odds_b1)

odds_b1
```

`STEP 8`: Repeat Steps 5-7 a total of 10 times (as per Multiple Imputation standards). Store the desired parameter and its variance-covariance matrix?

```{r}
CLQI_MI <- function(num_iterations = 10) {
  #repeat this 10 times and save the desired parameters... use purrr?
  
}

#return a dataframe with all necessary information
```


`STEP 9`: Aggregate with Rubin’s Rules

```{r}
#need a refresher, but should be easy
```


***************************************************************************************************************
Make sure we implement the other imputation methods! (especially fixed imputation)

Complete case: use `confounder_1_missing` and just ignore all NA values when doing regression

```{r}

```

Constant Imputation:

```{r}
missing_study <- missing_study |>
  mutate(confounder_1_constant_imputed = ifelse(is.na(confounder_1_missing), sqrt(2), confounder_1_missing))
```

Fixed Imputation:

```{r}

```


***************************************************************************************************************

Now we need to analyze the results, which will be another substantive section in this document
Here are functions for some other imputation algorithms for comparison!

```{r}
#bias example: take the aggregated b1 estimate, then subtract by the population b1 estimate to get bias value for one iteration
```


```{r}
#MSE example: need the variance of the b1 estimate... figure this one out!
```


```{r}
#coverage example: i have no clue !!!!!
```

